{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "PoQUPLtkjOpR",
        "outputId": "bac401ad-7c68-492f-a236-e7861800ded9"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Requirement already satisfied: pip in /usr/local/lib/python3.11/dist-packages (24.1.2)\n",
            "Collecting pip\n",
            "  Downloading pip-25.1.1-py3-none-any.whl.metadata (3.6 kB)\n",
            "Downloading pip-25.1.1-py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m32.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: pip\n",
            "  Attempting uninstall: pip\n",
            "    Found existing installation: pip 24.1.2\n",
            "    Uninstalling pip-24.1.2:\n",
            "      Successfully uninstalled pip-24.1.2\n",
            "Successfully installed pip-25.1.1\n",
            "Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (2.6.0+cu124)\n",
            "Collecting torch\n",
            "  Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (29 kB)\n",
            "Requirement already satisfied: torchvision in /usr/local/lib/python3.11/dist-packages (0.21.0+cu124)\n",
            "Collecting torchvision\n",
            "  Downloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (6.1 kB)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch) (4.14.0)\n",
            "Collecting sympy>=1.13.3 (from torch)\n",
            "  Downloading sympy-1.14.0-py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch) (2025.3.2)\n",
            "Collecting nvidia-cuda-nvrtc-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-runtime-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cuda-cupti-cu12==12.6.80 (from torch)\n",
            "  Downloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cudnn-cu12==9.5.1.17 (from torch)\n",
            "  Downloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cublas-cu12==12.6.4.1 (from torch)\n",
            "  Downloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufft-cu12==11.3.0.4 (from torch)\n",
            "  Downloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-curand-cu12==10.3.7.77 (from torch)\n",
            "  Downloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cusolver-cu12==11.7.1.2 (from torch)\n",
            "  Downloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparse-cu12==12.5.4.2 (from torch)\n",
            "  Downloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-cusparselt-cu12==0.6.3 (from torch)\n",
            "  Downloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting nvidia-nccl-cu12==2.26.2 (from torch)\n",
            "  Downloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (2.0 kB)\n",
            "Collecting nvidia-nvtx-cu12==12.6.77 (from torch)\n",
            "  Downloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.6 kB)\n",
            "Collecting nvidia-nvjitlink-cu12==12.6.85 (from torch)\n",
            "  Downloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting nvidia-cufile-cu12==1.11.1.6 (from torch)\n",
            "  Downloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl.metadata (1.5 kB)\n",
            "Collecting triton==3.3.1 (from torch)\n",
            "  Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (1.5 kB)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch) (75.2.0)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from torchvision) (2.0.2)\n",
            "Requirement already satisfied: pillow!=8.3.*,>=5.3.0 in /usr/local/lib/python3.11/dist-packages (from torchvision) (11.2.1)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch) (3.0.2)\n",
            "Downloading torch-2.7.1-cp311-cp311-manylinux_2_28_x86_64.whl (821.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m821.2/821.2 MB\u001b[0m \u001b[31m22.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cublas_cu12-12.6.4.1-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (393.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m393.1/393.1 MB\u001b[0m \u001b[31m35.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.6.80-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (8.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.9/8.9 MB\u001b[0m \u001b[31m148.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.6.77-py3-none-manylinux2014_x86_64.whl (23.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m155.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (897 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m897.7/897.7 kB\u001b[0m \u001b[31m50.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cudnn_cu12-9.5.1.17-py3-none-manylinux_2_28_x86_64.whl (571.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m571.0/571.0 MB\u001b[0m \u001b[31m48.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufft_cu12-11.3.0.4-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (200.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m200.2/200.2 MB\u001b[0m \u001b[31m71.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cufile_cu12-1.11.1.6-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m46.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_curand_cu12-10.3.7.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (56.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m68.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusolver_cu12-11.7.1.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (158.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m158.2/158.2 MB\u001b[0m \u001b[31m73.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparse_cu12-12.5.4.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (216.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m216.6/216.6 MB\u001b[0m \u001b[31m73.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_cusparselt_cu12-0.6.3-py3-none-manylinux2014_x86_64.whl (156.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m156.8/156.8 MB\u001b[0m \u001b[31m72.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nccl_cu12-2.26.2-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (201.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m201.3/201.3 MB\u001b[0m \u001b[31m71.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.6.85-py3-none-manylinux2010_x86_64.manylinux_2_12_x86_64.whl (19.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m19.7/19.7 MB\u001b[0m \u001b[31m132.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading nvidia_nvtx_cu12-12.6.77-py3-none-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (89 kB)\n",
            "Downloading triton-3.3.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (155.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m155.7/155.7 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading torchvision-0.22.1-cp311-cp311-manylinux_2_28_x86_64.whl (7.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7.5/7.5 MB\u001b[0m \u001b[31m108.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sympy-1.14.0-py3-none-any.whl (6.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.3/6.3 MB\u001b[0m \u001b[31m137.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: nvidia-cusparselt-cu12, triton, sympy, nvidia-nvtx-cu12, nvidia-nvjitlink-cu12, nvidia-nccl-cu12, nvidia-curand-cu12, nvidia-cufile-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, nvidia-cusparse-cu12, nvidia-cufft-cu12, nvidia-cudnn-cu12, nvidia-cusolver-cu12, torch, torchvision\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparselt-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparselt-cu12 0.6.2\n",
            "\u001b[2K    Uninstalling nvidia-cusparselt-cu12-0.6.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparselt-cu12-0.6.2\n",
            "\u001b[2K  Attempting uninstall: triton\n",
            "\u001b[2K    Found existing installation: triton 3.2.0\n",
            "\u001b[2K    Uninstalling triton-3.2.0:\n",
            "\u001b[2K      Successfully uninstalled triton-3.2.0\n",
            "\u001b[2K  Attempting uninstall: sympy\n",
            "\u001b[2K    Found existing installation: sympy 1.13.1\n",
            "\u001b[2K    Uninstalling sympy-1.13.1:\n",
            "\u001b[2K      Successfully uninstalled sympy-1.13.1\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvtx-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvtx-cu12 12.4.127\n",
            "\u001b[2K    Uninstalling nvidia-nvtx-cu12-12.4.127:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvtx-cu12-12.4.127\n",
            "\u001b[2K  Attempting uninstall: nvidia-nvjitlink-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-nccl-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-nccl-cu12 2.21.5\n",
            "\u001b[2K    Uninstalling nvidia-nccl-cu12-2.21.5:\n",
            "\u001b[2K      Successfully uninstalled nvidia-nccl-cu12-2.21.5\n",
            "\u001b[2K  Attempting uninstall: nvidia-curand-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-curand-cu12 10.3.6.82\n",
            "\u001b[2K    Uninstalling nvidia-curand-cu12-10.3.6.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-runtime-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cuda-cupti-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n",
            "\u001b[2K    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n",
            "\u001b[2K  Attempting uninstall: nvidia-cublas-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n",
            "\u001b[2K    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusparse-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n",
            "\u001b[2K    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n",
            "\u001b[2K  Attempting uninstall: nvidia-cufft-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n",
            "\u001b[2K    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n",
            "\u001b[2K  Attempting uninstall: nvidia-cudnn-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n",
            "\u001b[2K    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n",
            "\u001b[2K  Attempting uninstall: nvidia-cusolver-cu12\n",
            "\u001b[2K    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n",
            "\u001b[2K    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n",
            "\u001b[2K      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n",
            "\u001b[2K  Attempting uninstall: torch\n",
            "\u001b[2K    Found existing installation: torch 2.6.0+cu124\n",
            "\u001b[2K    Uninstalling torch-2.6.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torch-2.6.0+cu124\n",
            "\u001b[2K  Attempting uninstall: torchvision\n",
            "\u001b[2K    Found existing installation: torchvision 0.21.0+cu124\n",
            "\u001b[2K    Uninstalling torchvision-0.21.0+cu124:\n",
            "\u001b[2K      Successfully uninstalled torchvision-0.21.0+cu124\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18/18\u001b[0m [torchvision]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed nvidia-cublas-cu12-12.6.4.1 nvidia-cuda-cupti-cu12-12.6.80 nvidia-cuda-nvrtc-cu12-12.6.77 nvidia-cuda-runtime-cu12-12.6.77 nvidia-cudnn-cu12-9.5.1.17 nvidia-cufft-cu12-11.3.0.4 nvidia-cufile-cu12-1.11.1.6 nvidia-curand-cu12-10.3.7.77 nvidia-cusolver-cu12-11.7.1.2 nvidia-cusparse-cu12-12.5.4.2 nvidia-cusparselt-cu12-0.6.3 nvidia-nccl-cu12-2.26.2 nvidia-nvjitlink-cu12-12.6.85 nvidia-nvtx-cu12-12.6.77 sympy-1.14.0 torch-2.7.1 torchvision-0.22.1 triton-3.3.1\n",
            "Collecting numpy==1.24.4\n",
            "  Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.6 kB)\n",
            "Downloading numpy-1.24.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (17.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17.3/17.3 MB\u001b[0m \u001b[31m143.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy\n",
            "  Attempting uninstall: numpy\n",
            "    Found existing installation: numpy 2.0.2\n",
            "    Uninstalling numpy-2.0.2:\n",
            "      Successfully uninstalled numpy-2.0.2\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.24.4 which is incompatible.\n",
            "jax 0.5.2 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "treescope 0.1.9 requires numpy>=1.25.2, but you have numpy 1.24.4 which is incompatible.\n",
            "pymc 5.23.0 requires numpy>=1.25.0, but you have numpy 1.24.4 which is incompatible.\n",
            "blosc2 3.4.0 requires numpy>=1.26, but you have numpy 1.24.4 which is incompatible.\n",
            "jaxlib 0.5.1 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "xarray-einstats 0.9.0 requires numpy>=1.25, but you have numpy 1.24.4 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed numpy-1.24.4\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "893fe7182ffe428e8bff9e94e26c921b",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting protobuf==4.25.3\n",
            "  Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl.metadata (541 bytes)\n",
            "Downloading protobuf-4.25.3-cp37-abi3-manylinux2014_x86_64.whl (294 kB)\n",
            "Installing collected packages: protobuf\n",
            "  Attempting uninstall: protobuf\n",
            "    Found existing installation: protobuf 5.29.5\n",
            "    Uninstalling protobuf-5.29.5:\n",
            "      Successfully uninstalled protobuf-5.29.5\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "grpcio-status 1.71.0 requires protobuf<6.0dev,>=5.26.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 1.24.4 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed protobuf-4.25.3\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "d1d67d44a70a4b588b42b051cde7b459",
              "pip_warning": {
                "packages": [
                  "google"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting mediapipe==0.10.21\n",
            "  Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl.metadata (9.7 kB)\n",
            "Requirement already satisfied: absl-py in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (1.4.0)\n",
            "Requirement already satisfied: attrs>=19.1.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (25.3.0)\n",
            "Requirement already satisfied: flatbuffers>=2.0 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (25.2.10)\n",
            "Requirement already satisfied: jax in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (0.5.2)\n",
            "Requirement already satisfied: jaxlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (0.5.1)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (3.10.0)\n",
            "Requirement already satisfied: numpy<2 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (1.24.4)\n",
            "Requirement already satisfied: opencv-contrib-python in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (4.11.0.86)\n",
            "Requirement already satisfied: protobuf<5,>=4.25.3 in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (4.25.3)\n",
            "Collecting sounddevice>=0.4.4 (from mediapipe==0.10.21)\n",
            "  Downloading sounddevice-0.5.2-py3-none-any.whl.metadata (1.6 kB)\n",
            "Requirement already satisfied: sentencepiece in /usr/local/lib/python3.11/dist-packages (from mediapipe==0.10.21) (0.2.0)\n",
            "Requirement already satisfied: CFFI>=1.0 in /usr/local/lib/python3.11/dist-packages (from sounddevice>=0.4.4->mediapipe==0.10.21) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from CFFI>=1.0->sounddevice>=0.4.4->mediapipe==0.10.21) (2.22)\n",
            "Requirement already satisfied: ml_dtypes>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.21) (0.4.1)\n",
            "Collecting numpy<2 (from mediapipe==0.10.21)\n",
            "  Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (61 kB)\n",
            "Requirement already satisfied: opt_einsum in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.21) (3.4.0)\n",
            "Requirement already satisfied: scipy>=1.11.1 in /usr/local/lib/python3.11/dist-packages (from jax->mediapipe==0.10.21) (1.15.3)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (24.2)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->mediapipe==0.10.21) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->mediapipe==0.10.21) (1.17.0)\n",
            "Downloading mediapipe-0.10.21-cp311-cp311-manylinux_2_28_x86_64.whl (35.6 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.6/35.6 MB\u001b[0m \u001b[31m138.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading sounddevice-0.5.2-py3-none-any.whl (32 kB)\n",
            "Downloading numpy-1.26.4-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (18.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m18.3/18.3 MB\u001b[0m \u001b[31m216.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: numpy, sounddevice, mediapipe\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 1.24.4\n",
            "\u001b[2K    Uninstalling numpy-1.24.4:\n",
            "\u001b[2K      Successfully uninstalled numpy-1.24.4\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m3/3\u001b[0m [mediapipe]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed mediapipe-0.10.21 numpy-1.26.4 sounddevice-0.5.2\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "38e5aa5e8a534efcb30de420a7442ed4",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting pytorchvideo\n",
            "  Downloading pytorchvideo-0.1.5.tar.gz (132 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting fvcore (from pytorchvideo)\n",
            "  Downloading fvcore-0.1.5.post20221221.tar.gz (50 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting av (from pytorchvideo)\n",
            "  Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Collecting parameterized (from pytorchvideo)\n",
            "  Downloading parameterized-0.9.0-py2.py3-none-any.whl.metadata (18 kB)\n",
            "Collecting iopath (from pytorchvideo)\n",
            "  Downloading iopath-0.1.10.tar.gz (42 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from pytorchvideo) (3.5)\n",
            "Requirement already satisfied: numpy in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (1.26.4)\n",
            "Collecting yacs>=0.1.6 (from fvcore->pytorchvideo)\n",
            "  Downloading yacs-0.1.8-py3-none-any.whl.metadata (639 bytes)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (6.0.2)\n",
            "Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (4.67.1)\n",
            "Requirement already satisfied: termcolor>=1.1 in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (3.1.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (11.2.1)\n",
            "Requirement already satisfied: tabulate in /usr/local/lib/python3.11/dist-packages (from fvcore->pytorchvideo) (0.9.0)\n",
            "Requirement already satisfied: typing_extensions in /usr/local/lib/python3.11/dist-packages (from iopath->pytorchvideo) (4.14.0)\n",
            "Collecting portalocker (from iopath->pytorchvideo)\n",
            "  Downloading portalocker-3.2.0-py3-none-any.whl.metadata (8.7 kB)\n",
            "Downloading av-14.4.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (35.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m35.3/35.3 MB\u001b[0m \u001b[31m87.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading yacs-0.1.8-py3-none-any.whl (14 kB)\n",
            "Downloading parameterized-0.9.0-py2.py3-none-any.whl (20 kB)\n",
            "Downloading portalocker-3.2.0-py3-none-any.whl (22 kB)\n",
            "Building wheels for collected packages: pytorchvideo, fvcore, iopath\n",
            "\u001b[33m  DEPRECATION: Building 'pytorchvideo' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'pytorchvideo'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for pytorchvideo (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pytorchvideo: filename=pytorchvideo-0.1.5-py3-none-any.whl size=188686 sha256=2efcd1fe48fdbc3aa66654975d544bcb46a504c0ab7236339a7187d4b7702006\n",
            "  Stored in directory: /root/.cache/pip/wheels/a4/6d/ae/d016375a73be141a0e11bb42289e2d0b046c35687fc8010ecc\n",
            "\u001b[33m  DEPRECATION: Building 'fvcore' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'fvcore'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for fvcore (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for fvcore: filename=fvcore-0.1.5.post20221221-py3-none-any.whl size=61397 sha256=12649d7920ed5f9a609a8af29958d3466ac25c866e07bd65c969d8c25b26304c\n",
            "  Stored in directory: /root/.cache/pip/wheels/65/71/95/3b8fde5c65c6e4a806e0867c1651dcc71a1cb2f3430e8f355f\n",
            "\u001b[33m  DEPRECATION: Building 'iopath' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'iopath'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for iopath (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for iopath: filename=iopath-0.1.10-py3-none-any.whl size=31527 sha256=eb4c3b891d4b4ef71b5a690329000605502bb304eb1d9dcb791c840286263454\n",
            "  Stored in directory: /root/.cache/pip/wheels/ba/5e/16/6117f8fe7e9c0c161a795e10d94645ebcf301ccbd01f66d8ec\n",
            "Successfully built pytorchvideo fvcore iopath\n",
            "Installing collected packages: yacs, portalocker, parameterized, av, iopath, fvcore, pytorchvideo\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m7/7\u001b[0m [pytorchvideo]\n",
            "\u001b[1A\u001b[2KSuccessfully installed av-14.4.0 fvcore-0.1.5.post20221221 iopath-0.1.10 parameterized-0.9.0 portalocker-3.2.0 pytorchvideo-0.1.5 yacs-0.1.8\n",
            "Collecting ffmpeg-python\n",
            "  Downloading ffmpeg_python-0.2.0-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: future in /usr/local/lib/python3.11/dist-packages (from ffmpeg-python) (1.0.0)\n",
            "Downloading ffmpeg_python-0.2.0-py3-none-any.whl (25 kB)\n",
            "Installing collected packages: ffmpeg-python\n",
            "Successfully installed ffmpeg-python-0.2.0\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (0.11.0)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa) (2.22)\n",
            "Collecting panns_inference\n",
            "  Downloading panns_inference-0.1.1-py3-none-any.whl.metadata (2.4 kB)\n",
            "Requirement already satisfied: matplotlib in /usr/local/lib/python3.11/dist-packages (from panns_inference) (3.10.0)\n",
            "Requirement already satisfied: librosa in /usr/local/lib/python3.11/dist-packages (from panns_inference) (0.11.0)\n",
            "Collecting torchlibrosa (from panns_inference)\n",
            "  Downloading torchlibrosa-0.1.0-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: audioread>=2.1.9 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (3.0.1)\n",
            "Requirement already satisfied: numba>=0.51.0 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (0.60.0)\n",
            "Requirement already satisfied: numpy>=1.22.3 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (1.26.4)\n",
            "Requirement already satisfied: scipy>=1.6.0 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (1.15.3)\n",
            "Requirement already satisfied: scikit-learn>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (1.6.1)\n",
            "Requirement already satisfied: joblib>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (1.5.1)\n",
            "Requirement already satisfied: decorator>=4.3.0 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (4.4.2)\n",
            "Requirement already satisfied: soundfile>=0.12.1 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (0.13.1)\n",
            "Requirement already satisfied: pooch>=1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (1.8.2)\n",
            "Requirement already satisfied: soxr>=0.3.2 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (0.5.0.post1)\n",
            "Requirement already satisfied: typing_extensions>=4.1.1 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (4.14.0)\n",
            "Requirement already satisfied: lazy_loader>=0.1 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (0.4)\n",
            "Requirement already satisfied: msgpack>=1.0 in /usr/local/lib/python3.11/dist-packages (from librosa->panns_inference) (1.1.1)\n",
            "Requirement already satisfied: packaging in /usr/local/lib/python3.11/dist-packages (from lazy_loader>=0.1->librosa->panns_inference) (24.2)\n",
            "Requirement already satisfied: llvmlite<0.44,>=0.43.0dev0 in /usr/local/lib/python3.11/dist-packages (from numba>=0.51.0->librosa->panns_inference) (0.43.0)\n",
            "Requirement already satisfied: platformdirs>=2.5.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->panns_inference) (4.3.8)\n",
            "Requirement already satisfied: requests>=2.19.0 in /usr/local/lib/python3.11/dist-packages (from pooch>=1.1->librosa->panns_inference) (2.32.3)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.19.0->pooch>=1.1->librosa->panns_inference) (2025.6.15)\n",
            "Requirement already satisfied: threadpoolctl>=3.1.0 in /usr/local/lib/python3.11/dist-packages (from scikit-learn>=1.1.0->librosa->panns_inference) (3.6.0)\n",
            "Requirement already satisfied: cffi>=1.0 in /usr/local/lib/python3.11/dist-packages (from soundfile>=0.12.1->librosa->panns_inference) (1.17.1)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0->soundfile>=0.12.1->librosa->panns_inference) (2.22)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns_inference) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns_inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns_inference) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns_inference) (1.4.8)\n",
            "Requirement already satisfied: pillow>=8 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns_inference) (11.2.1)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns_inference) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib->panns_inference) (2.9.0.post0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib->panns_inference) (1.17.0)\n",
            "Downloading panns_inference-0.1.1-py3-none-any.whl (8.3 kB)\n",
            "Downloading torchlibrosa-0.1.0-py3-none-any.whl (11 kB)\n",
            "Installing collected packages: torchlibrosa, panns_inference\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [panns_inference]\n",
            "\u001b[1A\u001b[2KSuccessfully installed panns_inference-0.1.1 torchlibrosa-0.1.0\n",
            "Collecting opencv-python-headless==4.7.0.72\n",
            "  Downloading opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (18 kB)\n",
            "Requirement already satisfied: numpy>=1.21.2 in /usr/local/lib/python3.11/dist-packages (from opencv-python-headless==4.7.0.72) (1.26.4)\n",
            "Downloading opencv_python_headless-4.7.0.72-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (49.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.2/49.2 MB\u001b[0m \u001b[31m63.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
            "  Attempting uninstall: opencv-python-headless\n",
            "    Found existing installation: opencv-python-headless 4.11.0.86\n",
            "    Uninstalling opencv-python-headless-4.11.0.86:\n",
            "      Successfully uninstalled opencv-python-headless-4.11.0.86\n",
            "\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.7.0.72 which is incompatible.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.7.0.72 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed opencv-python-headless-4.7.0.72\n",
            "Collecting ultralytics\n",
            "  Downloading ultralytics-8.3.158-py3-none-any.whl.metadata (37 kB)\n",
            "Requirement already satisfied: numpy>=1.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.26.4)\n",
            "Requirement already satisfied: matplotlib>=3.3.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (3.10.0)\n",
            "Requirement already satisfied: opencv-python>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.11.0.86)\n",
            "Requirement already satisfied: pillow>=7.1.2 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (11.2.1)\n",
            "Requirement already satisfied: pyyaml>=5.3.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (6.0.2)\n",
            "Requirement already satisfied: requests>=2.23.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.32.3)\n",
            "Requirement already satisfied: scipy>=1.4.1 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (1.15.3)\n",
            "Requirement already satisfied: torch>=1.8.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.7.1)\n",
            "Requirement already satisfied: torchvision>=0.9.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (0.22.1)\n",
            "Requirement already satisfied: tqdm>=4.64.0 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (4.67.1)\n",
            "Requirement already satisfied: psutil in /usr/local/lib/python3.11/dist-packages (from ultralytics) (5.9.5)\n",
            "Requirement already satisfied: py-cpuinfo in /usr/local/lib/python3.11/dist-packages (from ultralytics) (9.0.0)\n",
            "Requirement already satisfied: pandas>=1.1.4 in /usr/local/lib/python3.11/dist-packages (from ultralytics) (2.2.2)\n",
            "Collecting ultralytics-thop>=2.0.0 (from ultralytics)\n",
            "  Downloading ultralytics_thop-2.0.14-py3-none-any.whl.metadata (9.4 kB)\n",
            "Requirement already satisfied: contourpy>=1.0.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.3.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (1.4.8)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (24.2)\n",
            "Requirement already satisfied: pyparsing>=2.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (3.2.3)\n",
            "Requirement already satisfied: python-dateutil>=2.7 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.3.0->ultralytics) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas>=1.1.4->ultralytics) (2025.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil>=2.7->matplotlib>=3.3.0->ultralytics) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests>=2.23.0->ultralytics) (2025.6.15)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.18.0)\n",
            "Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (4.14.0)\n",
            "Requirement already satisfied: sympy>=1.13.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.14.0)\n",
            "Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.5)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.1.6)\n",
            "Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2025.3.2)\n",
            "Requirement already satisfied: nvidia-cuda-nvrtc-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-runtime-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-cuda-cupti-cu12==12.6.80 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.6.80)\n",
            "Requirement already satisfied: nvidia-cudnn-cu12==9.5.1.17 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (9.5.1.17)\n",
            "Requirement already satisfied: nvidia-cublas-cu12==12.6.4.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.6.4.1)\n",
            "Requirement already satisfied: nvidia-cufft-cu12==11.3.0.4 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.3.0.4)\n",
            "Requirement already satisfied: nvidia-curand-cu12==10.3.7.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (10.3.7.77)\n",
            "Requirement already satisfied: nvidia-cusolver-cu12==11.7.1.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (11.7.1.2)\n",
            "Requirement already satisfied: nvidia-cusparse-cu12==12.5.4.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.5.4.2)\n",
            "Requirement already satisfied: nvidia-cusparselt-cu12==0.6.3 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (0.6.3)\n",
            "Requirement already satisfied: nvidia-nccl-cu12==2.26.2 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (2.26.2)\n",
            "Requirement already satisfied: nvidia-nvtx-cu12==12.6.77 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.6.77)\n",
            "Requirement already satisfied: nvidia-nvjitlink-cu12==12.6.85 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (12.6.85)\n",
            "Requirement already satisfied: nvidia-cufile-cu12==1.11.1.6 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (1.11.1.6)\n",
            "Requirement already satisfied: triton==3.3.1 in /usr/local/lib/python3.11/dist-packages (from torch>=1.8.0->ultralytics) (3.3.1)\n",
            "Requirement already satisfied: setuptools>=40.8.0 in /usr/local/lib/python3.11/dist-packages (from triton==3.3.1->torch>=1.8.0->ultralytics) (75.2.0)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy>=1.13.3->torch>=1.8.0->ultralytics) (1.3.0)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch>=1.8.0->ultralytics) (3.0.2)\n",
            "Downloading ultralytics-8.3.158-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m49.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading ultralytics_thop-2.0.14-py3-none-any.whl (26 kB)\n",
            "Installing collected packages: ultralytics-thop, ultralytics\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2/2\u001b[0m [ultralytics]\n",
            "\u001b[1A\u001b[2KSuccessfully installed ultralytics-8.3.158 ultralytics-thop-2.0.14\n",
            "Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (4.52.4)\n",
            "Requirement already satisfied: diffusers in /usr/local/lib/python3.11/dist-packages (0.33.1)\n",
            "Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from transformers) (3.18.0)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.30.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.33.0)\n",
            "Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (1.26.4)\n",
            "Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers) (24.2)\n",
            "Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers) (6.0.2)\n",
            "Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers) (2024.11.6)\n",
            "Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers) (2.32.3)\n",
            "Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.21.1)\n",
            "Requirement already satisfied: safetensors>=0.4.3 in /usr/local/lib/python3.11/dist-packages (from transformers) (0.5.3)\n",
            "Requirement already satisfied: tqdm>=4.27 in /usr/local/lib/python3.11/dist-packages (from transformers) (4.67.1)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (2025.3.2)\n",
            "Requirement already satisfied: typing-extensions>=3.7.4.3 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (4.14.0)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.30.0->transformers) (1.1.3)\n",
            "Requirement already satisfied: importlib-metadata in /usr/local/lib/python3.11/dist-packages (from diffusers) (8.7.0)\n",
            "Requirement already satisfied: Pillow in /usr/local/lib/python3.11/dist-packages (from diffusers) (11.2.1)\n",
            "Requirement already satisfied: zipp>=3.20 in /usr/local/lib/python3.11/dist-packages (from importlib-metadata->diffusers) (3.23.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.4.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2.4.0)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers) (2025.6.15)\n",
            "Collecting inference\n",
            "  Downloading inference-0.51.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting aiortc~=1.9.0 (from inference)\n",
            "  Downloading aiortc-1.9.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.5 kB)\n",
            "Collecting APScheduler<4.0.0,>=3.10.1 (from inference)\n",
            "  Downloading APScheduler-3.11.0-py3-none-any.whl.metadata (6.4 kB)\n",
            "Collecting asyncua~=1.1.5 (from inference)\n",
            "  Downloading asyncua-1.1.6-py3-none-any.whl.metadata (9.6 kB)\n",
            "Requirement already satisfied: cachetools<6.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (5.5.2)\n",
            "Requirement already satisfied: cython~=3.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (3.0.12)\n",
            "Collecting python-dotenv~=1.0.0 (from inference)\n",
            "  Downloading python_dotenv-1.0.1-py3-none-any.whl.metadata (23 kB)\n",
            "Collecting fastapi<0.111,>=0.100 (from inference)\n",
            "  Downloading fastapi-0.110.3-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting numpy<2.3.0,>=2.0.0 (from inference)\n",
            "  Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (62 kB)\n",
            "Collecting opencv-python<=4.10.0.84,>=4.8.1.78 (from inference)\n",
            "  Downloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Collecting opencv-contrib-python<=4.10.0.84,>=4.8.1.78 (from inference)\n",
            "  Downloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (20 kB)\n",
            "Requirement already satisfied: pillow<12.0,>=11.0 in /usr/local/lib/python3.11/dist-packages (from inference) (11.2.1)\n",
            "Collecting prometheus-fastapi-instrumentator<=6.0.0 (from inference)\n",
            "  Downloading prometheus_fastapi_instrumentator-6.0.0-py3-none-any.whl.metadata (13 kB)\n",
            "Collecting redis~=5.0.0 (from inference)\n",
            "  Downloading redis-5.0.8-py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: requests<3.0.0,>=2.32.0 in /usr/local/lib/python3.11/dist-packages (from inference) (2.32.3)\n",
            "Requirement already satisfied: rich<13.10.0,>=13.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (13.9.4)\n",
            "Collecting supervision<=0.30.0,>=0.25.1 (from inference)\n",
            "  Downloading supervision-0.25.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting pybase64~=1.0.0 (from inference)\n",
            "  Downloading pybase64-1.0.2.tar.gz (105 kB)\n",
            "  Installing build dependencies ... \u001b[?25l\u001b[?25hdone\n",
            "  Getting requirements to build wheel ... \u001b[?25l\u001b[?25hdone\n",
            "  Preparing metadata (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: scikit-image<=0.25.2,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from inference) (0.25.2)\n",
            "Requirement already satisfied: requests-toolbelt~=1.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (1.0.0)\n",
            "Requirement already satisfied: wheel<=0.45.1,>=0.38.1 in /usr/local/lib/python3.11/dist-packages (from inference) (0.45.1)\n",
            "Requirement already satisfied: setuptools>=70.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (75.2.0)\n",
            "Requirement already satisfied: networkx<4.0.0,>=3.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (3.5)\n",
            "Requirement already satisfied: pydantic<2.12.0,>=2.8.0 in /usr/local/lib/python3.11/dist-packages (from inference) (2.11.7)\n",
            "Collecting pydantic-settings<2.8 (from inference)\n",
            "  Downloading pydantic_settings-2.7.1-py3-none-any.whl.metadata (3.5 kB)\n",
            "Requirement already satisfied: openai<2.0.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from inference) (1.86.0)\n",
            "Collecting structlog<25.0.0,>=24.1.0 (from inference)\n",
            "  Downloading structlog-24.4.0-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting zxing-cpp~=2.2.0 (from inference)\n",
            "  Downloading zxing_cpp-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (1.9 kB)\n",
            "Collecting boto3<=1.35.60 (from inference)\n",
            "  Downloading boto3-1.35.60-py3-none-any.whl.metadata (6.7 kB)\n",
            "Collecting typing_extensions<=4.12.2,>=4.8.0 (from inference)\n",
            "  Downloading typing_extensions-4.12.2-py3-none-any.whl.metadata (3.0 kB)\n",
            "Collecting pydot~=2.0.0 (from inference)\n",
            "  Downloading pydot-2.0.0-py3-none-any.whl.metadata (9.6 kB)\n",
            "Collecting shapely<2.1.0,>=2.0.4 (from inference)\n",
            "  Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.8 kB)\n",
            "Collecting tldextract~=5.1.2 (from inference)\n",
            "  Downloading tldextract-5.1.3-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: packaging~=24.0 in /usr/local/lib/python3.11/dist-packages (from inference) (24.2)\n",
            "Collecting anthropic~=0.49.0 (from inference)\n",
            "  Downloading anthropic-0.49.0-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting pandas<3.0.0,>=2.2.3 (from inference)\n",
            "  Downloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (91 kB)\n",
            "Collecting paho-mqtt~=1.6.1 (from inference)\n",
            "  Downloading paho-mqtt-1.6.1.tar.gz (99 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Requirement already satisfied: pytest<9.0.0,>=8.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (8.3.5)\n",
            "Requirement already satisfied: tokenizers<0.22.0,>=0.19.0 in /usr/local/lib/python3.11/dist-packages (from inference) (0.21.1)\n",
            "Collecting slack-sdk~=3.33.4 (from inference)\n",
            "  Downloading slack_sdk-3.33.5-py2.py3-none-any.whl.metadata (15 kB)\n",
            "Collecting twilio~=9.3.7 (from inference)\n",
            "  Downloading twilio-9.3.8-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: httpx~=0.28.1 in /usr/local/lib/python3.11/dist-packages (from inference) (0.28.1)\n",
            "Collecting pylogix==1.0.5 (from inference)\n",
            "  Downloading pylogix-1.0.5-py2.py3-none-any.whl.metadata (5.6 kB)\n",
            "Collecting pymodbus<=3.8.3,>=3.6.9 (from inference)\n",
            "  Downloading pymodbus-3.8.3-py3-none-any.whl.metadata (15 kB)\n",
            "Collecting backoff~=2.2.0 (from inference)\n",
            "  Downloading backoff-2.2.1-py3-none-any.whl.metadata (14 kB)\n",
            "Collecting filelock<=3.17.0,>=3.12.0 (from inference)\n",
            "  Downloading filelock-3.17.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting onvif-zeep-async==2.0.0 (from inference)\n",
            "  Downloading onvif_zeep_async-2.0.0-py2.py3-none-any.whl.metadata (5.4 kB)\n",
            "Collecting simple-pid~=2.0.1 (from inference)\n",
            "  Downloading simple_pid-2.0.1-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting onnxruntime<1.22.0,>=1.15.1 (from inference)\n",
            "  Downloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl.metadata (4.5 kB)\n",
            "Requirement already satisfied: nvidia-ml-py<13.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (12.575.51)\n",
            "Collecting docker<8.0.0,>=7.0.0 (from inference)\n",
            "  Downloading docker-7.1.0-py3-none-any.whl.metadata (3.8 kB)\n",
            "Collecting click<8.2.0 (from inference)\n",
            "  Downloading click-8.1.8-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting typer<=0.12.5,>=0.9.0 (from inference)\n",
            "  Downloading typer-0.12.5-py3-none-any.whl.metadata (15 kB)\n",
            "Requirement already satisfied: PyYAML~=6.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (6.0.2)\n",
            "Requirement already satisfied: tqdm<5.0.0,>=4.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (4.67.1)\n",
            "Requirement already satisfied: py-cpuinfo~=9.0.0 in /usr/local/lib/python3.11/dist-packages (from inference) (9.0.0)\n",
            "Collecting aiohttp<=3.10.11,>=3.9.0 (from inference)\n",
            "  Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (7.7 kB)\n",
            "Collecting dataclasses-json~=0.6.0 (from inference)\n",
            "  Downloading dataclasses_json-0.6.7-py3-none-any.whl.metadata (25 kB)\n",
            "Collecting zeep<5.0.0,>=4.2.1 (from zeep[async]<5.0.0,>=4.2.1->onvif-zeep-async==2.0.0->inference)\n",
            "  Downloading zeep-4.3.1-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (2.6.1)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (1.3.2)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (25.3.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (1.7.0)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (6.4.4)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp<=3.10.11,>=3.9.0->inference) (1.20.1)\n",
            "Collecting aioice<1.0.0,>=0.9.0 (from aiortc~=1.9.0->inference)\n",
            "  Downloading aioice-0.10.1-py3-none-any.whl.metadata (4.1 kB)\n",
            "Collecting av<13.0.0,>=9.0.0 (from aiortc~=1.9.0->inference)\n",
            "  Downloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.6 kB)\n",
            "Requirement already satisfied: cffi>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from aiortc~=1.9.0->inference) (1.17.1)\n",
            "Requirement already satisfied: cryptography>=42.0.0 in /usr/local/lib/python3.11/dist-packages (from aiortc~=1.9.0->inference) (43.0.3)\n",
            "Requirement already satisfied: google-crc32c>=1.1 in /usr/local/lib/python3.11/dist-packages (from aiortc~=1.9.0->inference) (1.7.1)\n",
            "Collecting pyee>=9.0.0 (from aiortc~=1.9.0->inference)\n",
            "  Downloading pyee-13.0.0-py3-none-any.whl.metadata (2.9 kB)\n",
            "Collecting pylibsrtp>=0.10.0 (from aiortc~=1.9.0->inference)\n",
            "  Downloading pylibsrtp-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (4.1 kB)\n",
            "Requirement already satisfied: pyopenssl>=24.0.0 in /usr/local/lib/python3.11/dist-packages (from aiortc~=1.9.0->inference) (24.2.1)\n",
            "Collecting dnspython>=2.0.0 (from aioice<1.0.0,>=0.9.0->aiortc~=1.9.0->inference)\n",
            "  Downloading dnspython-2.7.0-py3-none-any.whl.metadata (5.8 kB)\n",
            "Collecting ifaddr>=0.2.0 (from aioice<1.0.0,>=0.9.0->aiortc~=1.9.0->inference)\n",
            "  Downloading ifaddr-0.2.0-py3-none-any.whl.metadata (4.9 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.11/dist-packages (from anthropic~=0.49.0->inference) (4.9.0)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/local/lib/python3.11/dist-packages (from anthropic~=0.49.0->inference) (1.9.0)\n",
            "Requirement already satisfied: jiter<1,>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from anthropic~=0.49.0->inference) (0.10.0)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.11/dist-packages (from anthropic~=0.49.0->inference) (1.3.1)\n",
            "Requirement already satisfied: idna>=2.8 in /usr/local/lib/python3.11/dist-packages (from anyio<5,>=3.5.0->anthropic~=0.49.0->inference) (3.10)\n",
            "Requirement already satisfied: tzlocal>=3.0 in /usr/local/lib/python3.11/dist-packages (from APScheduler<4.0.0,>=3.10.1->inference) (5.3.1)\n",
            "Requirement already satisfied: aiofiles in /usr/local/lib/python3.11/dist-packages (from asyncua~=1.1.5->inference) (24.1.0)\n",
            "Collecting aiosqlite (from asyncua~=1.1.5->inference)\n",
            "  Downloading aiosqlite-0.21.0-py3-none-any.whl.metadata (4.3 kB)\n",
            "Requirement already satisfied: python-dateutil in /usr/local/lib/python3.11/dist-packages (from asyncua~=1.1.5->inference) (2.9.0.post0)\n",
            "Requirement already satisfied: pytz in /usr/local/lib/python3.11/dist-packages (from asyncua~=1.1.5->inference) (2025.2)\n",
            "Requirement already satisfied: sortedcontainers in /usr/local/lib/python3.11/dist-packages (from asyncua~=1.1.5->inference) (2.4.0)\n",
            "Collecting wait-for2==0.3.2 (from asyncua~=1.1.5->inference)\n",
            "  Downloading wait_for2-0.3.2.tar.gz (16 kB)\n",
            "  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Collecting botocore<1.36.0,>=1.35.60 (from boto3<=1.35.60->inference)\n",
            "  Downloading botocore-1.35.99-py3-none-any.whl.metadata (5.7 kB)\n",
            "Collecting jmespath<2.0.0,>=0.7.1 (from boto3<=1.35.60->inference)\n",
            "  Downloading jmespath-1.0.1-py3-none-any.whl.metadata (7.6 kB)\n",
            "Collecting s3transfer<0.11.0,>=0.10.0 (from boto3<=1.35.60->inference)\n",
            "  Downloading s3transfer-0.10.4-py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: urllib3!=2.2.0,<3,>=1.25.4 in /usr/local/lib/python3.11/dist-packages (from botocore<1.36.0,>=1.35.60->boto3<=1.35.60->inference) (2.4.0)\n",
            "Collecting marshmallow<4.0.0,>=3.18.0 (from dataclasses-json~=0.6.0->inference)\n",
            "  Downloading marshmallow-3.26.1-py3-none-any.whl.metadata (7.3 kB)\n",
            "Collecting typing-inspect<1,>=0.4.0 (from dataclasses-json~=0.6.0->inference)\n",
            "  Downloading typing_inspect-0.9.0-py3-none-any.whl.metadata (1.5 kB)\n",
            "Collecting starlette<0.38.0,>=0.37.2 (from fastapi<0.111,>=0.100->inference)\n",
            "  Downloading starlette-0.37.2-py3-none-any.whl.metadata (5.9 kB)\n",
            "Requirement already satisfied: certifi in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.1->inference) (2025.6.15)\n",
            "Requirement already satisfied: httpcore==1.* in /usr/local/lib/python3.11/dist-packages (from httpx~=0.28.1->inference) (1.0.9)\n",
            "Requirement already satisfied: h11>=0.16 in /usr/local/lib/python3.11/dist-packages (from httpcore==1.*->httpx~=0.28.1->inference) (0.16.0)\n",
            "Collecting coloredlogs (from onnxruntime<1.22.0,>=1.15.1->inference)\n",
            "  Downloading coloredlogs-15.0.1-py2.py3-none-any.whl.metadata (12 kB)\n",
            "Requirement already satisfied: flatbuffers in /usr/local/lib/python3.11/dist-packages (from onnxruntime<1.22.0,>=1.15.1->inference) (25.2.10)\n",
            "Requirement already satisfied: protobuf in /usr/local/lib/python3.11/dist-packages (from onnxruntime<1.22.0,>=1.15.1->inference) (4.25.3)\n",
            "Requirement already satisfied: sympy in /usr/local/lib/python3.11/dist-packages (from onnxruntime<1.22.0,>=1.15.1->inference) (1.14.0)\n",
            "Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas<3.0.0,>=2.2.3->inference) (2025.2)\n",
            "Requirement already satisfied: prometheus-client<1.0.0,>=0.8.0 in /usr/local/lib/python3.11/dist-packages (from prometheus-fastapi-instrumentator<=6.0.0->inference) (0.22.1)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12.0,>=2.8.0->inference) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.33.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12.0,>=2.8.0->inference) (2.33.2)\n",
            "Requirement already satisfied: typing-inspection>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<2.12.0,>=2.8.0->inference) (0.4.1)\n",
            "Requirement already satisfied: pyparsing>=3 in /usr/local/lib/python3.11/dist-packages (from pydot~=2.0.0->inference) (3.2.3)\n",
            "Requirement already satisfied: iniconfig in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.0.0->inference) (2.1.0)\n",
            "Requirement already satisfied: pluggy<2,>=1.5 in /usr/local/lib/python3.11/dist-packages (from pytest<9.0.0,>=8.0.0->inference) (1.6.0)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.11/dist-packages (from python-dateutil->asyncua~=1.1.5->inference) (1.17.0)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests<3.0.0,>=2.32.0->inference) (3.4.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.11/dist-packages (from rich<13.10.0,>=13.0.0->inference) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.11/dist-packages (from rich<13.10.0,>=13.0.0->inference) (2.19.1)\n",
            "Requirement already satisfied: scipy>=1.11.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image<=0.25.2,>=0.19.0->inference) (1.15.3)\n",
            "Requirement already satisfied: imageio!=2.35.0,>=2.33 in /usr/local/lib/python3.11/dist-packages (from scikit-image<=0.25.2,>=0.19.0->inference) (2.37.0)\n",
            "Requirement already satisfied: tifffile>=2022.8.12 in /usr/local/lib/python3.11/dist-packages (from scikit-image<=0.25.2,>=0.19.0->inference) (2025.6.11)\n",
            "Requirement already satisfied: lazy-loader>=0.4 in /usr/local/lib/python3.11/dist-packages (from scikit-image<=0.25.2,>=0.19.0->inference) (0.4)\n",
            "Requirement already satisfied: contourpy>=1.0.7 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference) (1.3.2)\n",
            "Requirement already satisfied: defusedxml<0.8.0,>=0.7.1 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference) (0.7.1)\n",
            "Requirement already satisfied: matplotlib>=3.6.0 in /usr/local/lib/python3.11/dist-packages (from supervision<=0.30.0,>=0.25.1->inference) (3.10.0)\n",
            "Collecting requests-file>=1.4 (from tldextract~=5.1.2->inference)\n",
            "  Downloading requests_file-2.1.0-py2.py3-none-any.whl.metadata (1.7 kB)\n",
            "Requirement already satisfied: huggingface-hub<1.0,>=0.16.4 in /usr/local/lib/python3.11/dist-packages (from tokenizers<0.22.0,>=0.19.0->inference) (0.33.0)\n",
            "Requirement already satisfied: fsspec>=2023.5.0 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.22.0,>=0.19.0->inference) (2025.3.2)\n",
            "Requirement already satisfied: hf-xet<2.0.0,>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from huggingface-hub<1.0,>=0.16.4->tokenizers<0.22.0,>=0.19.0->inference) (1.1.3)\n",
            "Requirement already satisfied: PyJWT<3.0.0,>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from twilio~=9.3.7->inference) (2.10.1)\n",
            "Collecting aiohttp-retry==2.8.3 (from twilio~=9.3.7->inference)\n",
            "  Downloading aiohttp_retry-2.8.3-py3-none-any.whl.metadata (8.9 kB)\n",
            "Requirement already satisfied: shellingham>=1.3.0 in /usr/local/lib/python3.11/dist-packages (from typer<=0.12.5,>=0.9.0->inference) (1.5.4)\n",
            "Collecting mypy-extensions>=0.3.0 (from typing-inspect<1,>=0.4.0->dataclasses-json~=0.6.0->inference)\n",
            "  Downloading mypy_extensions-1.1.0-py3-none-any.whl.metadata (1.1 kB)\n",
            "Requirement already satisfied: propcache>=0.2.1 in /usr/local/lib/python3.11/dist-packages (from yarl<2.0,>=1.12.0->aiohttp<=3.10.11,>=3.9.0->inference) (0.3.2)\n",
            "Collecting isodate>=0.5.4 (from zeep<5.0.0,>=4.2.1->zeep[async]<5.0.0,>=4.2.1->onvif-zeep-async==2.0.0->inference)\n",
            "  Downloading isodate-0.7.2-py3-none-any.whl.metadata (11 kB)\n",
            "Requirement already satisfied: lxml>=4.6.0 in /usr/local/lib/python3.11/dist-packages (from zeep<5.0.0,>=4.2.1->zeep[async]<5.0.0,>=4.2.1->onvif-zeep-async==2.0.0->inference) (5.4.0)\n",
            "Requirement already satisfied: platformdirs>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from zeep<5.0.0,>=4.2.1->zeep[async]<5.0.0,>=4.2.1->onvif-zeep-async==2.0.0->inference) (4.3.8)\n",
            "Requirement already satisfied: pycparser in /usr/local/lib/python3.11/dist-packages (from cffi>=1.0.0->aiortc~=1.9.0->inference) (2.22)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.11/dist-packages (from markdown-it-py>=2.2.0->rich<13.10.0,>=13.0.0->inference) (0.1.2)\n",
            "Requirement already satisfied: cycler>=0.10 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference) (0.12.1)\n",
            "Requirement already satisfied: fonttools>=4.22.0 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference) (4.58.4)\n",
            "Requirement already satisfied: kiwisolver>=1.3.1 in /usr/local/lib/python3.11/dist-packages (from matplotlib>=3.6.0->supervision<=0.30.0,>=0.25.1->inference) (1.4.8)\n",
            "Collecting humanfriendly>=9.1 (from coloredlogs->onnxruntime<1.22.0,>=1.15.1->inference)\n",
            "  Downloading humanfriendly-10.0-py2.py3-none-any.whl.metadata (9.2 kB)\n",
            "Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy->onnxruntime<1.22.0,>=1.15.1->inference) (1.3.0)\n",
            "Downloading inference-0.51.0-py3-none-any.whl (2.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.4/2.4 MB\u001b[0m \u001b[31m86.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onvif_zeep_async-2.0.0-py2.py3-none-any.whl (190 kB)\n",
            "Downloading pylogix-1.0.5-py2.py3-none-any.whl (75 kB)\n",
            "Downloading aiohttp-3.10.11-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.3/1.3 MB\u001b[0m \u001b[31m73.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiortc-1.9.0-cp38-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.9 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m98.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aioice-0.10.1-py3-none-any.whl (24 kB)\n",
            "Downloading anthropic-0.49.0-py3-none-any.whl (243 kB)\n",
            "Downloading APScheduler-3.11.0-py3-none-any.whl (64 kB)\n",
            "Downloading asyncua-1.1.6-py3-none-any.whl (1.1 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.1/1.1 MB\u001b[0m \u001b[31m61.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading av-12.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (34.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m34.4/34.4 MB\u001b[0m \u001b[31m67.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading backoff-2.2.1-py3-none-any.whl (15 kB)\n",
            "Downloading boto3-1.35.60-py3-none-any.whl (139 kB)\n",
            "Downloading botocore-1.35.99-py3-none-any.whl (13.3 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.3/13.3 MB\u001b[0m \u001b[31m158.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading click-8.1.8-py3-none-any.whl (98 kB)\n",
            "Downloading dataclasses_json-0.6.7-py3-none-any.whl (28 kB)\n",
            "Downloading docker-7.1.0-py3-none-any.whl (147 kB)\n",
            "Downloading fastapi-0.110.3-py3-none-any.whl (91 kB)\n",
            "Downloading filelock-3.17.0-py3-none-any.whl (16 kB)\n",
            "Downloading jmespath-1.0.1-py3-none-any.whl (20 kB)\n",
            "Downloading marshmallow-3.26.1-py3-none-any.whl (50 kB)\n",
            "Downloading numpy-2.2.6-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (16.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.8/16.8 MB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading onnxruntime-1.21.1-cp311-cp311-manylinux_2_27_x86_64.manylinux_2_28_x86_64.whl (16.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m16.0/16.0 MB\u001b[0m \u001b[31m104.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_contrib_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (68.7 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m68.7/68.7 MB\u001b[0m \u001b[31m51.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading opencv_python-4.10.0.84-cp37-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (62.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.5/62.5 MB\u001b[0m \u001b[31m74.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading pandas-2.3.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (12.4 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m12.4/12.4 MB\u001b[0m \u001b[31m145.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading prometheus_fastapi_instrumentator-6.0.0-py3-none-any.whl (18 kB)\n",
            "Downloading pydantic_settings-2.7.1-py3-none-any.whl (29 kB)\n",
            "Downloading pydot-2.0.0-py3-none-any.whl (22 kB)\n",
            "Downloading pymodbus-3.8.3-py3-none-any.whl (160 kB)\n",
            "Downloading python_dotenv-1.0.1-py3-none-any.whl (19 kB)\n",
            "Downloading redis-5.0.8-py3-none-any.whl (255 kB)\n",
            "Downloading s3transfer-0.10.4-py3-none-any.whl (83 kB)\n",
            "Downloading shapely-2.0.7-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.5 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.5/2.5 MB\u001b[0m \u001b[31m102.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading simple_pid-2.0.1-py3-none-any.whl (7.2 kB)\n",
            "Downloading slack_sdk-3.33.5-py2.py3-none-any.whl (292 kB)\n",
            "Downloading starlette-0.37.2-py3-none-any.whl (71 kB)\n",
            "Downloading structlog-24.4.0-py3-none-any.whl (67 kB)\n",
            "Downloading supervision-0.25.1-py3-none-any.whl (181 kB)\n",
            "Downloading tldextract-5.1.3-py3-none-any.whl (104 kB)\n",
            "Downloading twilio-9.3.8-py2.py3-none-any.whl (1.8 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.8/1.8 MB\u001b[0m \u001b[31m90.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading aiohttp_retry-2.8.3-py3-none-any.whl (9.8 kB)\n",
            "Downloading typer-0.12.5-py3-none-any.whl (47 kB)\n",
            "Downloading typing_extensions-4.12.2-py3-none-any.whl (37 kB)\n",
            "Downloading typing_inspect-0.9.0-py3-none-any.whl (8.8 kB)\n",
            "Downloading zeep-4.3.1-py3-none-any.whl (101 kB)\n",
            "Downloading zxing_cpp-2.2.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (941 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m941.4/941.4 kB\u001b[0m \u001b[31m58.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading dnspython-2.7.0-py3-none-any.whl (313 kB)\n",
            "Downloading ifaddr-0.2.0-py3-none-any.whl (12 kB)\n",
            "Downloading isodate-0.7.2-py3-none-any.whl (22 kB)\n",
            "Downloading mypy_extensions-1.1.0-py3-none-any.whl (5.0 kB)\n",
            "Downloading pyee-13.0.0-py3-none-any.whl (15 kB)\n",
            "Downloading pylibsrtp-0.12.0-cp39-abi3-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (2.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.2/2.2 MB\u001b[0m \u001b[31m90.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_file-2.1.0-py2.py3-none-any.whl (4.2 kB)\n",
            "Downloading aiosqlite-0.21.0-py3-none-any.whl (15 kB)\n",
            "Downloading coloredlogs-15.0.1-py2.py3-none-any.whl (46 kB)\n",
            "Downloading humanfriendly-10.0-py2.py3-none-any.whl (86 kB)\n",
            "Building wheels for collected packages: wait-for2, paho-mqtt, pybase64\n",
            "\u001b[33m  DEPRECATION: Building 'wait-for2' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'wait-for2'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for wait-for2 (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for wait-for2: filename=wait_for2-0.3.2-py3-none-any.whl size=10491 sha256=b64734e3f07d1ce84157e6f810f1759fa0ce74932365b1a19b3bc2fe5b02e200\n",
            "  Stored in directory: /root/.cache/pip/wheels/ac/cb/01/822df6dee993980ee967de65e69427f5dee2362e85484632a4\n",
            "\u001b[33m  DEPRECATION: Building 'paho-mqtt' using the legacy setup.py bdist_wheel mechanism, which will be removed in a future version. pip 25.3 will enforce this behaviour change. A possible replacement is to use the standardized build interface by setting the `--use-pep517` option, (possibly combined with `--no-build-isolation`), or adding a `pyproject.toml` file to the source tree of 'paho-mqtt'. Discussion can be found at https://github.com/pypa/pip/issues/6334\u001b[0m\u001b[33m\n",
            "\u001b[0m  Building wheel for paho-mqtt (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for paho-mqtt: filename=paho_mqtt-1.6.1-py3-none-any.whl size=62116 sha256=afcf01fdc670d2a7fd662b1e20e747ca6b95a873b38063fff4d9e33143028bdd\n",
            "  Stored in directory: /root/.cache/pip/wheels/29/ea/a5/ba9a63aaf4cd4e16e8a87ee31fb4d11b04ff5e1735d312619a\n",
            "  Building wheel for pybase64 (pyproject.toml) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for pybase64: filename=pybase64-1.0.2-cp311-cp311-linux_x86_64.whl size=146812 sha256=36710faec9ee594d8a400f90c73253ae2dc203d1e7848a7af353681a06b2a393\n",
            "  Stored in directory: /root/.cache/pip/wheels/3d/c0/15/5e79fe7fd3f97ebaf4c080e7c337fb06c1d01bc61905dcb26b\n",
            "Successfully built wait-for2 paho-mqtt pybase64\n",
            "Installing collected packages: wait-for2, pylogix, paho-mqtt, ifaddr, zxing-cpp, typing_extensions, structlog, slack-sdk, simple-pid, redis, python-dotenv, pymodbus, pydot, pybase64, numpy, mypy-extensions, marshmallow, jmespath, isodate, humanfriendly, filelock, dnspython, click, backoff, av, APScheduler, typing-inspect, shapely, requests-file, pylibsrtp, pyee, pandas, opencv-python, opencv-contrib-python, docker, coloredlogs, botocore, aiosqlite, aioice, aiohttp, zeep, typer, tldextract, starlette, s3transfer, onnxruntime, dataclasses-json, aiohttp-retry, twilio, supervision, pydantic-settings, fastapi, boto3, asyncua, anthropic, aiortc, prometheus-fastapi-instrumentator, onvif-zeep-async, inference\n",
            "\u001b[2K  Attempting uninstall: typing_extensions\n",
            "\u001b[2K    Found existing installation: typing_extensions 4.14.0\n",
            "\u001b[2K    Uninstalling typing_extensions-4.14.0:\n",
            "\u001b[2K      Successfully uninstalled typing_extensions-4.14.0\n",
            "\u001b[2K  Attempting uninstall: pydot\n",
            "\u001b[2K    Found existing installation: pydot 3.0.4\n",
            "\u001b[2K    Uninstalling pydot-3.0.4:\n",
            "\u001b[2K      Successfully uninstalled pydot-3.0.4\n",
            "\u001b[2K  Attempting uninstall: numpy\n",
            "\u001b[2K    Found existing installation: numpy 1.26.4\n",
            "\u001b[2K    Uninstalling numpy-1.26.4:\n",
            "\u001b[2K      Successfully uninstalled numpy-1.26.4\n",
            "\u001b[2K  Attempting uninstall: filelock\n",
            "\u001b[2K    Found existing installation: filelock 3.18.0\n",
            "\u001b[2K    Uninstalling filelock-3.18.0:\n",
            "\u001b[2K      Successfully uninstalled filelock-3.18.0\n",
            "\u001b[2K  Attempting uninstall: click\n",
            "\u001b[2K    Found existing installation: click 8.2.1\n",
            "\u001b[2K    Uninstalling click-8.2.1:\n",
            "\u001b[2K      Successfully uninstalled click-8.2.1\n",
            "\u001b[2K  Attempting uninstall: av\n",
            "\u001b[2K    Found existing installation: av 14.4.0\n",
            "\u001b[2K    Uninstalling av-14.4.0:\n",
            "\u001b[2K      Successfully uninstalled av-14.4.0\n",
            "\u001b[2K  Attempting uninstall: shapely\n",
            "\u001b[2K    Found existing installation: shapely 2.1.1\n",
            "\u001b[2K    Uninstalling shapely-2.1.1:\n",
            "\u001b[2K      Successfully uninstalled shapely-2.1.1\n",
            "\u001b[2K  Attempting uninstall: pandas\n",
            "\u001b[2K    Found existing installation: pandas 2.2.2\n",
            "\u001b[2K    Uninstalling pandas-2.2.2:\n",
            "\u001b[2K      Successfully uninstalled pandas-2.2.2\n",
            "\u001b[2K  Attempting uninstall: opencv-python\n",
            "\u001b[2K    Found existing installation: opencv-python 4.11.0.86\n",
            "\u001b[2K    Uninstalling opencv-python-4.11.0.86:\n",
            "\u001b[2K      Successfully uninstalled opencv-python-4.11.0.86\n",
            "\u001b[2K  Attempting uninstall: opencv-contrib-python\n",
            "\u001b[2K    Found existing installation: opencv-contrib-python 4.11.0.86\n",
            "\u001b[2K    Uninstalling opencv-contrib-python-4.11.0.86:\n",
            "\u001b[2K      Successfully uninstalled opencv-contrib-python-4.11.0.86\n",
            "\u001b[2K  Attempting uninstall: aiohttp\n",
            "\u001b[2K    Found existing installation: aiohttp 3.11.15\n",
            "\u001b[2K    Uninstalling aiohttp-3.11.15:\n",
            "\u001b[2K      Successfully uninstalled aiohttp-3.11.15\n",
            "\u001b[2K  Attempting uninstall: typer\n",
            "\u001b[2K    Found existing installation: typer 0.16.0\n",
            "\u001b[2K    Uninstalling typer-0.16.0:\n",
            "\u001b[2K      Successfully uninstalled typer-0.16.0\n",
            "\u001b[2K  Attempting uninstall: starlette\n",
            "\u001b[2K    Found existing installation: starlette 0.46.2\n",
            "\u001b[2K    Uninstalling starlette-0.46.2:\n",
            "\u001b[2K      Successfully uninstalled starlette-0.46.2\n",
            "\u001b[2K  Attempting uninstall: fastapi\n",
            "\u001b[2K    Found existing installation: fastapi 0.115.12\n",
            "\u001b[2K    Uninstalling fastapi-0.115.12:\n",
            "\u001b[2K      Successfully uninstalled fastapi-0.115.12\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59/59\u001b[0m [inference]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "mediapipe 0.10.21 requires numpy<2, but you have numpy 2.2.6 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "numba 0.60.0 requires numpy<2.1,>=1.22, but you have numpy 2.2.6 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.7.1 which is incompatible.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.7.0.72 which is incompatible.\n",
            "typeguard 4.4.3 requires typing_extensions>=4.14.0, but you have typing-extensions 4.12.2 which is incompatible.\n",
            "fastai 2.7.19 requires torch<2.7,>=1.10, but you have torch 2.7.1 which is incompatible.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.7.0.72 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "gradio 5.31.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.110.3 which is incompatible.\n",
            "gradio 5.31.0 requires starlette<1.0,>=0.40.0; sys_platform != \"emscripten\", but you have starlette 0.37.2 which is incompatible.\n",
            "tensorflow 2.18.0 requires numpy<2.1.0,>=1.26.0, but you have numpy 2.2.6 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0mSuccessfully installed APScheduler-3.11.0 aiohttp-3.10.11 aiohttp-retry-2.8.3 aioice-0.10.1 aiortc-1.9.0 aiosqlite-0.21.0 anthropic-0.49.0 asyncua-1.1.6 av-12.3.0 backoff-2.2.1 boto3-1.35.60 botocore-1.35.99 click-8.1.8 coloredlogs-15.0.1 dataclasses-json-0.6.7 dnspython-2.7.0 docker-7.1.0 fastapi-0.110.3 filelock-3.17.0 humanfriendly-10.0 ifaddr-0.2.0 inference-0.51.0 isodate-0.7.2 jmespath-1.0.1 marshmallow-3.26.1 mypy-extensions-1.1.0 numpy-2.2.6 onnxruntime-1.21.1 onvif-zeep-async-2.0.0 opencv-contrib-python-4.10.0.84 opencv-python-4.10.0.84 paho-mqtt-1.6.1 pandas-2.3.0 prometheus-fastapi-instrumentator-6.0.0 pybase64-1.0.2 pydantic-settings-2.7.1 pydot-2.0.0 pyee-13.0.0 pylibsrtp-0.12.0 pylogix-1.0.5 pymodbus-3.8.3 python-dotenv-1.0.1 redis-5.0.8 requests-file-2.1.0 s3transfer-0.10.4 shapely-2.0.7 simple-pid-2.0.1 slack-sdk-3.33.5 starlette-0.37.2 structlog-24.4.0 supervision-0.25.1 tldextract-5.1.3 twilio-9.3.8 typer-0.12.5 typing-inspect-0.9.0 typing_extensions-4.12.2 wait-for2-0.3.2 zeep-4.3.1 zxing-cpp-2.2.0\n"
          ]
        },
        {
          "data": {
            "application/vnd.colab-display-data+json": {
              "id": "f21da6a4b1244223af3060d901b18d9c",
              "pip_warning": {
                "packages": [
                  "numpy"
                ]
              }
            }
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[?25l   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/1.9 MB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.9/1.9 MB\u001b[0m \u001b[31m83.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m4.5/4.5 MB\u001b[0m \u001b[31m125.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m755.6/755.6 MB\u001b[0m \u001b[31m29.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m410.6/410.6 MB\u001b[0m \u001b[31m56.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m14.1/14.1 MB\u001b[0m \u001b[31m144.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m23.7/23.7 MB\u001b[0m \u001b[31m132.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m823.6/823.6 kB\u001b[0m \u001b[31m44.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m731.7/731.7 MB\u001b[0m \u001b[31m24.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m121.6/121.6 MB\u001b[0m \u001b[31m59.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.5/56.5 MB\u001b[0m \u001b[31m61.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m124.2/124.2 MB\u001b[0m \u001b[31m63.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m196.0/196.0 MB\u001b[0m \u001b[31m70.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m166.0/166.0 MB\u001b[0m \u001b[31m68.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m167.9/167.9 MB\u001b[0m \u001b[31m65.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m109.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m17/17\u001b[0m [facenet-pytorch]\n",
            "\u001b[1A\u001b[2K\u001b[31mERROR: pip's dependency resolver does not currently take into account all the packages that are installed. This behaviour is the source of the following dependency conflicts.\n",
            "inference 0.51.0 requires numpy<2.3.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "inference 0.51.0 requires pillow<12.0,>=11.0, but you have pillow 10.2.0 which is incompatible.\n",
            "google-colab 1.0.0 requires pandas==2.2.2, but you have pandas 2.3.0 which is incompatible.\n",
            "thinc 8.3.6 requires numpy<3.0.0,>=2.0.0, but you have numpy 1.26.4 which is incompatible.\n",
            "cudf-cu12 25.2.1 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "ydf 0.12.0 requires protobuf<6.0.0,>=5.29.1, but you have protobuf 4.25.3 which is incompatible.\n",
            "torchaudio 2.6.0+cu124 requires torch==2.6.0, but you have torch 2.2.2 which is incompatible.\n",
            "albucore 0.0.24 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.7.0.72 which is incompatible.\n",
            "albumentations 2.0.8 requires opencv-python-headless>=4.9.0.80, but you have opencv-python-headless 4.7.0.72 which is incompatible.\n",
            "dask-cudf-cu12 25.2.2 requires pandas<2.2.4dev0,>=2.0, but you have pandas 2.3.0 which is incompatible.\n",
            "gradio 5.31.0 requires fastapi<1.0,>=0.115.2, but you have fastapi 0.110.3 which is incompatible.\n",
            "gradio 5.31.0 requires starlette<1.0,>=0.40.0; sys_platform != \"emscripten\", but you have starlette 0.37.2 which is incompatible.\u001b[0m\u001b[31m\n",
            "\u001b[0m"
          ]
        }
      ],
      "source": [
        "# 1. Upgrade pip first\n",
        "!pip install --upgrade pip\n",
        "\n",
        "# 2. Install PyTorch + torchvision (Colab usually has them, but let's ensure latest compatible)\n",
        "!pip install torch torchvision --upgrade\n",
        "\n",
        "# 3. Install numpy compatible with mediapipe and other libs (avoid numpy 2.0+)\n",
        "!pip install numpy==1.24.4\n",
        "\n",
        "# 4. Install protobuf version compatible with mediapipe\n",
        "!pip install protobuf==4.25.3\n",
        "\n",
        "# 5. Install mediapipe (works with numpy < 2 and protobuf <5)\n",
        "!pip install mediapipe==0.10.21\n",
        "\n",
        "# 6. Install pytorchvideo (latest stable)\n",
        "!pip install pytorchvideo\n",
        "\n",
        "# 7. Install ffmpeg-python (Python bindings for ffmpeg)\n",
        "!pip install ffmpeg-python\n",
        "\n",
        "# 8. Install librosa\n",
        "!pip install librosa\n",
        "\n",
        "# 9. Install panns_inference\n",
        "!pip install panns_inference\n",
        "\n",
        "# 10. Install OpenCV\n",
        "!pip install opencv-python-headless==4.7.0.72\n",
        "\n",
        "# 11. Install ultralytics (YOLOv8)\n",
        "!pip install ultralytics\n",
        "\n",
        "# 12. Install transformers and diffusers for generative AI\n",
        "!pip install transformers diffusers\n",
        "\n",
        "!pip install inference\n",
        "!pip install facenet-pytorch --quiet\n",
        "# 13. Restart runtime manually after installation to apply changes properly\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R3hz5fchjVFf",
        "outputId": "7e10e6e1-6416-451f-c455-1858451e567d"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:282: ModelDependencyMissing: Your `inference` configuration does not support PaliGemma model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:341: ModelDependencyMissing: Your `inference` configuration does not support Qwen2.5-VL model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:353: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:363: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:374: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[clip]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:386: ModelDependencyMissing: Your `inference` configuration does not support OWLv2 model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:412: ModelDependencyMissing: Your `inference` configuration does not support SmolVLM2.Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:425: ModelDependencyMissing: Your `inference` configuration does not support Depth Estimation.Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:456: ModelDependencyMissing: Your `inference` configuration does not support TrOCR model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:467: ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:478: ModelDependencyMissing: Your `inference` configuration does not support YoloWorld model. Use pip install 'inference[yolo-world]' to install missing requirements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "fatal: destination path 'fire-and-smoke-detection-yolov8' already exists and is not an empty directory.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:491: ModelDependencyMissing: Your `inference` configuration does not support Perception Encoder.Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "mv: cannot stat '/content/fire-and-smoke-detection-yolov8/weights/best.pt': No such file or directory\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "from torchvision.transforms import Normalize  # ✅ Use this\n",
        "from pytorchvideo.models.hub import x3d_s\n",
        "from torchvision.io import read_video\n",
        "import urllib.request\n",
        "import ffmpeg\n",
        "import librosa\n",
        "from panns_inference import AudioTagging, labels\n",
        "import cv2\n",
        "import torch\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "from inference import get_model\n",
        "from facenet_pytorch import MTCNN\n",
        "!git clone https://github.com/luminous0219/fire-and-smoke-detection-yolov8.git\n",
        "!mv /content/fire-and-smoke-detection-yolov8/weights/best.pt /content/\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "id": "pMSCK2htka8n"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "from torchvision.transforms import Normalize  # ✅ Use this\n",
        "from pytorchvideo.models.hub import x3d_s\n",
        "from torchvision.io import read_video\n",
        "import urllib.request\n",
        "def action_recognition_model(text,video_path_final):\n",
        "  # Load pretrained X3D model\n",
        "  model = x3d_s(pretrained=True)\n",
        "  model.eval()\n",
        "\n",
        "  # Load a sample video file (replace with your crime scene video path)\n",
        "  video_path = video_path_final\n",
        "\n",
        "  # Read video frames\n",
        "  video, _, _ = read_video(video_path, pts_unit='sec')\n",
        "\n",
        "  frames_per_clip = 16\n",
        "  total_frames = video.shape[0]\n",
        "  indices = torch.linspace(0, total_frames - 1, frames_per_clip).long()\n",
        "  clip = video[indices]\n",
        "\n",
        "  # Convert (T, H, W, C) to (C, T, H, W) and scale\n",
        "  clip = clip.permute(3, 0, 1, 2).float() / 255.0\n",
        "\n",
        "  # Normalize using pytorchvideo.transforms.Normalize (expects (C,T,H,W))\n",
        "  mean = [0.45, 0.45, 0.45]\n",
        "  std = [0.225, 0.225, 0.225]\n",
        "  normalize = Normalize(mean=mean, std=std)\n",
        "\n",
        "  # Normalize each frame (T) individually\n",
        "  clip = torch.stack([normalize(frame) for frame in clip.permute(1, 0, 2, 3)], dim=1)\n",
        "  clip = clip.unsqueeze(0)  # Add batch dimension\n",
        "\n",
        "  # Run inference\n",
        "  with torch.no_grad():\n",
        "      preds = model(clip)\n",
        "\n",
        "  # Load kinetics labels\n",
        "  labels_url = \"https://raw.githubusercontent.com/deepmind/kinetics-i3d/master/data/label_map.txt\"\n",
        "  labels_path = \"kinetics400_labels.txt\"\n",
        "  urllib.request.urlretrieve(labels_url, labels_path)\n",
        "\n",
        "  with open(labels_path, \"r\") as f:\n",
        "      kinetics_labels = [line.strip() for line in f.readlines()]\n",
        "\n",
        "  # Crime keywords (lowercase)\n",
        "  crime_keywords = [\n",
        "      \"fighting\", \"punching\", \"kicking\", \"wrestling\", \"shooting gun\",\n",
        "      \"handcuffing\", \"chasing\", \"assaulting\", \"stabbing\", \"stealing\",\n",
        "      \"kidnapping\", \"breaking\", \"robbery\", \"arresting\"\n",
        "  ]\n",
        "\n",
        "  # Find indices of crime-related labels\n",
        "  crime_indices = [\n",
        "      i for i, label in enumerate(kinetics_labels)\n",
        "      if any(keyword in label.lower() for keyword in crime_keywords)\n",
        "  ]\n",
        "\n",
        "  if not crime_indices:\n",
        "      print(\"No crime-related labels found in kinetics labels.\")\n",
        "  else:\n",
        "      # Select preds only for crime-related classes\n",
        "      crime_preds = preds[0, crime_indices]\n",
        "\n",
        "      # Compute softmax probabilities on crime classes\n",
        "      crime_probs = torch.nn.functional.softmax(crime_preds, dim=0)\n",
        "\n",
        "      # Sort descending by probability\n",
        "      sorted_probs, sorted_idx = torch.sort(crime_probs, descending=True)\n",
        "\n",
        "      print(\"Crime-related predictions (sorted):\")\n",
        "      text+=\"Crime-related predictions (sorted):\\n\"\n",
        "      for prob, idx in zip(sorted_probs, sorted_idx):\n",
        "          label = kinetics_labels[crime_indices[idx]]\n",
        "          print(f\"{label}: {prob.item() * 100:.2f}%\")\n",
        "          text+=f\"{label}: {prob.item() * 100:.2f}%\\n\"\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "metadata": {
        "id": "BhtdsmZb7oVN"
      },
      "outputs": [],
      "source": [
        "import librosa\n",
        "from panns_inference import AudioTagging, labels\n",
        "import ffmpeg\n",
        "import os\n",
        "\n",
        "def audio_classification_model(text,video_path_final):\n",
        "    video_file = video_path_final\n",
        "    audio_file = \"audio_extracted.wav\"\n",
        "\n",
        "    try:\n",
        "        # Extract audio from video\n",
        "        ffmpeg.input(video_file).output(audio_file, ac=1, ar=32000).run(overwrite_output=True)\n",
        "        print(f\"Extracted audio saved as: {audio_file}\")\n",
        "\n",
        "        # Load audio file safely\n",
        "        if not os.path.exists(audio_file) or os.path.getsize(audio_file) == 0:\n",
        "            raise ValueError(\"Audio file is empty or missing.\")\n",
        "\n",
        "        audio, _ = librosa.load(audio_file, sr=32000, mono=True)\n",
        "\n",
        "        # Check if audio is silent or has negligible data\n",
        "        if audio is None or len(audio) == 0 or max(audio) < 1e-5:\n",
        "            raise ValueError(\"Audio file contains no useful audio.\")\n",
        "\n",
        "        audio = audio[None, :]  # Add batch dimension\n",
        "\n",
        "        # Load model\n",
        "        at = AudioTagging(checkpoint_path=None, device='cpu')\n",
        "\n",
        "        # Run inference\n",
        "        clipwise_output, _ = at.inference(audio)\n",
        "        clipwise_output = clipwise_output[0]  # numpy array with shape (527,)\n",
        "\n",
        "        # Define crime keywords\n",
        "        crime_keywords = [\n",
        "            \"shout\", \"yell\", \"scream\", \"grunt\",\n",
        "            \"fire\", \"explosion\", \"Boom\", \"gunshot\", \"gun fire\", \"gun\",\n",
        "            \"machine gun\", \"artillery fire\", \"cap gun\",\n",
        "            \"alarm\", \"siren\", \"emergency vehicle\",\n",
        "            \"police car\", \"ambulance\", \"fire engine\", \"fire truck\",\n",
        "            \"vehicle\", \"car\", \"truck\"\n",
        "        ]\n",
        "        crime_keywords = [k.lower() for k in crime_keywords]\n",
        "\n",
        "        # Filter relevant indices\n",
        "        filtered_indices = [\n",
        "            i for i, label in enumerate(labels)\n",
        "            if any(kw in label.lower() for kw in crime_keywords)\n",
        "        ]\n",
        "\n",
        "        print(f\"Filtered crime/emergency related labels count: {len(filtered_indices)}\")\n",
        "        print(\"\\nDetected crime/emergency events with confidence > 0.05:\")\n",
        "        text += \"\\nDetected crime/emergency events with confidence > 0.05:\\n\"\n",
        "\n",
        "        found = False\n",
        "        for i in filtered_indices:\n",
        "            prob = clipwise_output[i]\n",
        "            if prob > 0.05:\n",
        "                print(f\"{labels[i]}: {prob:.3f}\")\n",
        "                text += f\"{labels[i]}: {prob:.3f}\\n\"\n",
        "                found = True\n",
        "\n",
        "        if not found:\n",
        "            text += \"No relevant audio events detected above threshold.\\n\"\n",
        "\n",
        "    except Exception as e:\n",
        "        print(f\"[Audio Error] {e}\")\n",
        "        text += \"\\nNo audio detected or unable to process audio.\\n\"\n",
        "\n",
        "    return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "metadata": {
        "id": "N2zExnRiqZQ9"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "\n",
        "def pose_person_model(text,video_path_final):\n",
        "  # Load models\n",
        "  yolo = YOLO('yolov8n.pt')  # person detection\n",
        "  mp_pose = mp.solutions.pose.Pose(static_image_mode=False)\n",
        "\n",
        "  # Simple IOU function for tracking\n",
        "  def iou(bb_test, bb_gt):\n",
        "      xx1 = max(bb_test[0], bb_gt[0])\n",
        "      yy1 = max(bb_test[1], bb_gt[1])\n",
        "      xx2 = min(bb_test[2], bb_gt[2])\n",
        "      yy2 = min(bb_test[3], bb_gt[3])\n",
        "      w = max(0., xx2 - xx1)\n",
        "      h = max(0., yy2 - yy1)\n",
        "      inter = w * h\n",
        "      area1 = (bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1])\n",
        "      area2 = (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1])\n",
        "      o = inter / float(area1 + area2 - inter + 1e-6)\n",
        "      return o\n",
        "\n",
        "  class Sort:\n",
        "      def __init__(self, iou_threshold=0.3, max_lost=3):\n",
        "          self.iou_threshold = iou_threshold\n",
        "          self.max_lost = max_lost\n",
        "          self.tracks = dict()\n",
        "          self.next_id = 0\n",
        "\n",
        "      def update(self, detections):\n",
        "          if len(detections) == 0:\n",
        "              to_delete = []\n",
        "              for tid in list(self.tracks.keys()):\n",
        "                  self.tracks[tid]['lost'] += 1\n",
        "                  if self.tracks[tid]['lost'] > self.max_lost:\n",
        "                      to_delete.append(tid)\n",
        "              for tid in to_delete:\n",
        "                  del self.tracks[tid]\n",
        "              output = []\n",
        "              for tid, trk in self.tracks.items():\n",
        "                  bbox = trk['bbox']\n",
        "                  output.append([int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]), tid])\n",
        "              return output\n",
        "\n",
        "          if len(self.tracks) == 0:\n",
        "              for det in detections:\n",
        "                  self.tracks[self.next_id] = {'bbox': det[:4], 'lost': 0}\n",
        "                  self.next_id += 1\n",
        "              return [[*det[:4], tid] for tid, det in zip(range(self.next_id), detections)]\n",
        "\n",
        "          assigned_tracks = set()\n",
        "          assigned_detections = set()\n",
        "          updated_tracks = dict()\n",
        "\n",
        "          det_bboxes = np.array([det[:4] for det in detections])\n",
        "\n",
        "          track_ids = list(self.tracks.keys())\n",
        "          track_bboxes = np.array([self.tracks[tid]['bbox'] for tid in track_ids])\n",
        "\n",
        "          iou_matrix = np.zeros((len(track_bboxes), len(det_bboxes)), dtype=np.float32)\n",
        "          for t, tb in enumerate(track_bboxes):\n",
        "              for d, db in enumerate(det_bboxes):\n",
        "                  iou_matrix[t, d] = iou(tb, db)\n",
        "\n",
        "          for t, tid in enumerate(track_ids):\n",
        "              if iou_matrix.shape[1] == 0:\n",
        "                  break\n",
        "              d_best = np.argmax(iou_matrix[t])\n",
        "              if iou_matrix[t, d_best] >= self.iou_threshold and d_best not in assigned_detections:\n",
        "                  updated_tracks[tid] = {'bbox': detections[d_best][:4], 'lost': 0}\n",
        "                  assigned_tracks.add(tid)\n",
        "                  assigned_detections.add(d_best)\n",
        "\n",
        "          for tid in track_ids:\n",
        "              if tid not in assigned_tracks:\n",
        "                  self.tracks[tid]['lost'] += 1\n",
        "                  if self.tracks[tid]['lost'] <= self.max_lost:\n",
        "                      updated_tracks[tid] = self.tracks[tid]\n",
        "\n",
        "          for d, det in enumerate(detections):\n",
        "              if d not in assigned_detections:\n",
        "                  updated_tracks[self.next_id] = {'bbox': det[:4], 'lost': 0}\n",
        "                  self.next_id += 1\n",
        "\n",
        "          self.tracks = updated_tracks\n",
        "          output = []\n",
        "          for tid, trk in self.tracks.items():\n",
        "              bbox = trk['bbox']\n",
        "              output.append([int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]), tid])\n",
        "          return output\n",
        "\n",
        "  tracker = Sort()\n",
        "\n",
        "  # For loitering detection - store recent positions per ID\n",
        "  position_history = defaultdict(list)\n",
        "  LOITERING_FRAME_THRESHOLD = 15  # number of frames to consider\n",
        "  LOITERING_DIST_THRESHOLD = 20  # pixels threshold for minimal movement\n",
        "\n",
        "  def euclidean_dist(p1, p2):\n",
        "      return np.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
        "\n",
        "  def is_loitering(track_id, current_pos):\n",
        "      history = position_history[track_id]\n",
        "      history.append(current_pos)\n",
        "      if len(history) > LOITERING_FRAME_THRESHOLD:\n",
        "          history.pop(0)\n",
        "      if len(history) < LOITERING_FRAME_THRESHOLD:\n",
        "          return False\n",
        "      # Calculate max distance moved in history\n",
        "      dists = [euclidean_dist(history[i], history[i+1]) for i in range(len(history)-1)]\n",
        "      total_movement = sum(dists)\n",
        "      return total_movement < LOITERING_DIST_THRESHOLD\n",
        "\n",
        "  # Behavior classification with extended categories\n",
        "  def classify_behavior(pose_dict, bbox, track_id):\n",
        "      if not pose_dict:\n",
        "          return \"unknown\"\n",
        "\n",
        "      # Collapsed - large shoulder height difference\n",
        "      if \"left_shoulder\" in pose_dict and \"right_shoulder\" in pose_dict:\n",
        "          l_sh = pose_dict[\"left_shoulder\"]\n",
        "          r_sh = pose_dict[\"right_shoulder\"]\n",
        "          if abs(l_sh['y'] - r_sh['y']) > 50:\n",
        "              return \"collapsed\"\n",
        "\n",
        "      # Hands up - wrists above shoulders\n",
        "      if \"left_wrist\" in pose_dict and \"right_wrist\" in pose_dict and \\\n",
        "        \"left_shoulder\" in pose_dict and \"right_shoulder\" in pose_dict:\n",
        "          lw = pose_dict[\"left_wrist\"]\n",
        "          rw = pose_dict[\"right_wrist\"]\n",
        "          ls = pose_dict[\"left_shoulder\"]\n",
        "          rs = pose_dict[\"right_shoulder\"]\n",
        "          if lw['y'] < ls['y'] and rw['y'] < rs['y']:\n",
        "              return \"hands_up_pose\"\n",
        "\n",
        "      # Gun pointing - right arm extended forward (heuristic)\n",
        "      if \"right_wrist\" in pose_dict and \"right_elbow\" in pose_dict and \"right_shoulder\" in pose_dict:\n",
        "          rw = pose_dict[\"right_wrist\"]\n",
        "          re = pose_dict[\"right_elbow\"]\n",
        "          rs = pose_dict[\"right_shoulder\"]\n",
        "          if rw['x'] > re['x'] + 40 and abs(rw['y'] - re['y']) < 30 and abs(re['y'] - rs['y']) < 30:\n",
        "              return \"gun_pointing_pose\"\n",
        "\n",
        "      # Pointing pose - check both hands for consistent direction\n",
        "\n",
        "      import math\n",
        "\n",
        "      def distance(p1, p2):\n",
        "          return math.sqrt((p1[\"x\"] - p2[\"x\"])**2 + (p1[\"y\"] - p2[\"y\"])**2)\n",
        "\n",
        "      def angle_between(p1, p2, p3):\n",
        "          a = distance(p2, p3)\n",
        "          b = distance(p1, p3)\n",
        "          c = distance(p1, p2)\n",
        "          if a * c == 0:\n",
        "              return 180\n",
        "          cos_angle = (a**2 + c**2 - b**2) / (2 * a * c)\n",
        "          cos_angle = max(-1, min(1, cos_angle))\n",
        "          return math.degrees(math.acos(cos_angle))\n",
        "\n",
        "      def vector(p1, p2):\n",
        "          return (p2[\"x\"] - p1[\"x\"], p2[\"y\"] - p1[\"y\"])\n",
        "\n",
        "      def unit_vector(v):\n",
        "          mag = math.sqrt(v[0]*2 + v[1]*2)\n",
        "          if mag == 0:\n",
        "              return (0, 0)\n",
        "          return (v[0]/mag, v[1]/mag)\n",
        "\n",
        "      def dot(v1, v2):\n",
        "          return v1[0]*v2[0] + v1[1]*v2[1]\n",
        "      import math\n",
        "\n",
        "      def is_pointing_arm(shoulder, elbow, wrist, min_arm_length=80, min_angle=160, min_forward_x=30):\n",
        "          # Calculate arm lengths\n",
        "          upper_arm = distance(shoulder, elbow)\n",
        "          lower_arm = distance(elbow, wrist)\n",
        "          full_arm = distance(shoulder, wrist)\n",
        "\n",
        "          # Calculate angle between shoulder, elbow, and wrist\n",
        "          angle = angle_between(shoulder, elbow, wrist)\n",
        "\n",
        "          # Must be mostly straight\n",
        "          if angle < min_angle:\n",
        "              return False\n",
        "\n",
        "          # Must be long enough\n",
        "          if full_arm < min_arm_length:\n",
        "              return False\n",
        "\n",
        "          # Wrist should be in front of shoulder\n",
        "          if wrist[\"x\"] - shoulder[\"x\"] < min_forward_x:\n",
        "              return False\n",
        "\n",
        "          # NEW: Arm vector should not be pointing downward\n",
        "          dx = wrist[\"x\"] - shoulder[\"x\"]\n",
        "          dy = wrist[\"y\"] - shoulder[\"y\"]\n",
        "          arm_angle = math.degrees(math.atan2(dy, dx))  # angle in degrees\n",
        "\n",
        "          # Accept only horizontal to slightly upward arm angles (e.g., -60 to +60 degrees)\n",
        "          if arm_angle > 60 or arm_angle < -60:\n",
        "              return False\n",
        "\n",
        "          return True\n",
        "\n",
        "      # --- POINTING LOGIC ---\n",
        "      left_pointing = False\n",
        "      if all(k in pose_dict for k in [\"left_shoulder\", \"left_elbow\", \"left_wrist\"]):\n",
        "          # For left arm, wrist should be to the *left* of shoulder (i.e., smaller x value)\n",
        "          if is_pointing_arm(pose_dict[\"left_shoulder\"], pose_dict[\"left_elbow\"], pose_dict[\"left_wrist\"], min_forward_x=-30):\n",
        "              left_pointing = True\n",
        "\n",
        "      right_pointing = False\n",
        "      if all(k in pose_dict for k in [\"right_shoulder\", \"right_elbow\", \"right_wrist\"]):\n",
        "          # For right arm, wrist should be to the *right* of shoulder (i.e., larger x value)\n",
        "          if is_pointing_arm(pose_dict[\"right_shoulder\"], pose_dict[\"right_elbow\"], pose_dict[\"right_wrist\"], min_forward_x=30):\n",
        "              right_pointing = True\n",
        "\n",
        "      if left_pointing or right_pointing:\n",
        "          return \"pointing\"\n",
        "\n",
        "      # Loitering\n",
        "      x1, y1, x2, y2 = bbox\n",
        "      cx = int((x1 + x2) / 2)\n",
        "      cy = int((y1 + y2) / 2)\n",
        "      if is_loitering(track_id, (cx, cy)):\n",
        "          return \"loitering\"\n",
        "\n",
        "      return \"normal\"\n",
        "\n",
        "  # Video input/output setup\n",
        "  cap = cv2.VideoCapture(video_path_final)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  print(f\"Original video FPS: {fps}\")\n",
        "\n",
        "  target_fps = 2\n",
        "  frame_skip = int(round(fps / target_fps))\n",
        "  print(f\"Processing every {frame_skip} frame(s) to get ~{target_fps} FPS\")\n",
        "\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  out = cv2.VideoWriter('output_annotated.mp4', fourcc, target_fps, (width, height))\n",
        "\n",
        "  log_file = open(\"behavior_log.txt\", \"w\")\n",
        "  text+=\"Pose+person tracking model's output\"\n",
        "  frame_count = 0\n",
        "\n",
        "  while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "\n",
        "      if frame_count % frame_skip == 0:\n",
        "          results = yolo(frame)[0]\n",
        "          detections = []\n",
        "\n",
        "\n",
        "          for det in results.boxes.data.cpu().numpy():\n",
        "              x1, y1, x2, y2, conf, cls = det\n",
        "              if int(cls) == 0 and conf > 0.5:\n",
        "                  detections.append([x1, y1, x2, y2, conf])\n",
        "\n",
        "          tracks = tracker.update(detections)\n",
        "          output_dict = {}\n",
        "\n",
        "          for track in tracks:\n",
        "              x1, y1, x2, y2, track_id = map(int, track)\n",
        "              person_crop = frame[y1:y2, x1:x2]\n",
        "              h, w = person_crop.shape[:2]\n",
        "\n",
        "              if h == 0 or w == 0:\n",
        "                  continue\n",
        "\n",
        "              pose_result = mp_pose.process(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
        "\n",
        "              joint_data = {}\n",
        "\n",
        "              if pose_result.pose_landmarks:\n",
        "                  for idx, lm in enumerate(pose_result.pose_landmarks.landmark):\n",
        "                      joint_name = mp.solutions.pose.PoseLandmark(idx).name.lower()\n",
        "                      joint_data[joint_name] = {\n",
        "                          \"x\": int(x1 + lm.x * w),\n",
        "                          \"y\": int(y1 + lm.y * h),\n",
        "                          \"confidence\": round(lm.visibility, 2)\n",
        "                      }\n",
        "\n",
        "              behavior = classify_behavior(joint_data, (x1, y1, x2, y2), track_id)\n",
        "              output_dict[f\"id:{track_id}\"] = {\n",
        "                  # \"joints\": joint_data,\n",
        "                  \"behavior\": behavior\n",
        "              }\n",
        "              text+=f\"{track_id}:{behavior}\\n\"\n",
        "\n",
        "              # Draw bounding box + label\n",
        "              color = (0, 255, 0)  # green default\n",
        "              if behavior in [\"collapsed\", \"pointing\", \"hands_up_pose\", \"gun_pointing_pose\", \"aggressive\", \"loitering\"]:\n",
        "                  color = (0, 0, 255)  # red alert\n",
        "\n",
        "              cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "              label = f\"ID:{track_id} {behavior}\"\n",
        "              cv2.putText(frame, label, (x1, y1 - 10),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "          log_file.write(json.dumps(output_dict) + \"\\n\")\n",
        "          out.write(frame)\n",
        "          # cv2_imshow(frame)\n",
        "          # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "          #     break\n",
        "\n",
        "      frame_count += 1\n",
        "\n",
        "  cap.release()\n",
        "  out.release()\n",
        "  log_file.close()\n",
        "  # cv2.destroyAllWindows()\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "id": "14mJHwovDqqW"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "import torch\n",
        "import mediapipe as mp\n",
        "from ultralytics import YOLO\n",
        "from collections import defaultdict\n",
        "import json\n",
        "import time\n",
        "from google.colab.patches import cv2_imshow\n",
        "import numpy as np\n",
        "\n",
        "def pose_person_model(text,video_path_final):\n",
        "  # Load models\n",
        "  yolo = YOLO('yolov8n.pt')  # person detection\n",
        "  mp_pose = mp.solutions.pose.Pose(static_image_mode=False)\n",
        "\n",
        "  # Simple IOU function for tracking\n",
        "  def iou(bb_test, bb_gt):\n",
        "      xx1 = max(bb_test[0], bb_gt[0])\n",
        "      yy1 = max(bb_test[1], bb_gt[1])\n",
        "      xx2 = min(bb_test[2], bb_gt[2])\n",
        "      yy2 = min(bb_test[3], bb_gt[3])\n",
        "      w = max(0., xx2 - xx1)\n",
        "      h = max(0., yy2 - yy1)\n",
        "      inter = w * h\n",
        "      area1 = (bb_test[2] - bb_test[0]) * (bb_test[3] - bb_test[1])\n",
        "      area2 = (bb_gt[2] - bb_gt[0]) * (bb_gt[3] - bb_gt[1])\n",
        "      o = inter / float(area1 + area2 - inter + 1e-6)\n",
        "      return o\n",
        "  def get_detection_summary(results):\n",
        "    summary = {}\n",
        "    if results and results[0].boxes is not None:\n",
        "        for c in results[0].boxes.cls.tolist():\n",
        "            class_name = yolo.names[int(c)]\n",
        "            summary[class_name] = summary.get(class_name, 0) + 1\n",
        "    summary_text = ', '.join(f\"{v} {k}{'s' if v > 1 else ''}\" for k, v in summary.items())\n",
        "    return summary_text\n",
        "\n",
        "  class Sort:\n",
        "      def __init__(self, iou_threshold=0.3, max_lost=3):\n",
        "          self.iou_threshold = iou_threshold\n",
        "          self.max_lost = max_lost\n",
        "          self.tracks = dict()\n",
        "          self.next_id = 0\n",
        "\n",
        "      def update(self, detections):\n",
        "          if len(detections) == 0:\n",
        "              to_delete = []\n",
        "              for tid in list(self.tracks.keys()):\n",
        "                  self.tracks[tid]['lost'] += 1\n",
        "                  if self.tracks[tid]['lost'] > self.max_lost:\n",
        "                      to_delete.append(tid)\n",
        "              for tid in to_delete:\n",
        "                  del self.tracks[tid]\n",
        "              output = []\n",
        "              for tid, trk in self.tracks.items():\n",
        "                  bbox = trk['bbox']\n",
        "                  output.append([int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]), tid])\n",
        "              return output\n",
        "\n",
        "          if len(self.tracks) == 0:\n",
        "              for det in detections:\n",
        "                  self.tracks[self.next_id] = {'bbox': det[:4], 'lost': 0}\n",
        "                  self.next_id += 1\n",
        "              return [[*det[:4], tid] for tid, det in zip(range(self.next_id), detections)]\n",
        "\n",
        "          assigned_tracks = set()\n",
        "          assigned_detections = set()\n",
        "          updated_tracks = dict()\n",
        "\n",
        "          det_bboxes = np.array([det[:4] for det in detections])\n",
        "\n",
        "          track_ids = list(self.tracks.keys())\n",
        "          track_bboxes = np.array([self.tracks[tid]['bbox'] for tid in track_ids])\n",
        "\n",
        "          iou_matrix = np.zeros((len(track_bboxes), len(det_bboxes)), dtype=np.float32)\n",
        "          for t, tb in enumerate(track_bboxes):\n",
        "              for d, db in enumerate(det_bboxes):\n",
        "                  iou_matrix[t, d] = iou(tb, db)\n",
        "\n",
        "          for t, tid in enumerate(track_ids):\n",
        "              if iou_matrix.shape[1] == 0:\n",
        "                  break\n",
        "              d_best = np.argmax(iou_matrix[t])\n",
        "              if iou_matrix[t, d_best] >= self.iou_threshold and d_best not in assigned_detections:\n",
        "                  updated_tracks[tid] = {'bbox': detections[d_best][:4], 'lost': 0}\n",
        "                  assigned_tracks.add(tid)\n",
        "                  assigned_detections.add(d_best)\n",
        "\n",
        "          for tid in track_ids:\n",
        "              if tid not in assigned_tracks:\n",
        "                  self.tracks[tid]['lost'] += 1\n",
        "                  if self.tracks[tid]['lost'] <= self.max_lost:\n",
        "                      updated_tracks[tid] = self.tracks[tid]\n",
        "\n",
        "          for d, det in enumerate(detections):\n",
        "              if d not in assigned_detections:\n",
        "                  updated_tracks[self.next_id] = {'bbox': det[:4], 'lost': 0}\n",
        "                  self.next_id += 1\n",
        "\n",
        "          self.tracks = updated_tracks\n",
        "          output = []\n",
        "          for tid, trk in self.tracks.items():\n",
        "              bbox = trk['bbox']\n",
        "              output.append([int(bbox[0]), int(bbox[1]), int(bbox[2]), int(bbox[3]), tid])\n",
        "          return output\n",
        "\n",
        "  tracker = Sort()\n",
        "\n",
        "  # For loitering detection - store recent positions per ID\n",
        "  position_history = defaultdict(list)\n",
        "  LOITERING_FRAME_THRESHOLD = 15  # number of frames to consider\n",
        "  LOITERING_DIST_THRESHOLD = 20  # pixels threshold for minimal movement\n",
        "\n",
        "  def euclidean_dist(p1, p2):\n",
        "      return np.sqrt((p1[0]-p2[0])**2 + (p1[1]-p2[1])**2)\n",
        "\n",
        "  def is_loitering(track_id, current_pos):\n",
        "      history = position_history[track_id]\n",
        "      history.append(current_pos)\n",
        "      if len(history) > LOITERING_FRAME_THRESHOLD:\n",
        "          history.pop(0)\n",
        "      if len(history) < LOITERING_FRAME_THRESHOLD:\n",
        "          return False\n",
        "      # Calculate max distance moved in history\n",
        "      dists = [euclidean_dist(history[i], history[i+1]) for i in range(len(history)-1)]\n",
        "      total_movement = sum(dists)\n",
        "      return total_movement < LOITERING_DIST_THRESHOLD\n",
        "\n",
        "  # Behavior classification with extended categories\n",
        "  def classify_behavior(pose_dict, bbox, track_id):\n",
        "      if not pose_dict:\n",
        "          return \"unknown\"\n",
        "\n",
        "      # Collapsed - large shoulder height difference\n",
        "      if \"left_shoulder\" in pose_dict and \"right_shoulder\" in pose_dict:\n",
        "          l_sh = pose_dict[\"left_shoulder\"]\n",
        "          r_sh = pose_dict[\"right_shoulder\"]\n",
        "          if abs(l_sh['y'] - r_sh['y']) > 50:\n",
        "              return \"collapsed\"\n",
        "\n",
        "      # Hands up - wrists above shoulders\n",
        "      if \"left_wrist\" in pose_dict and \"right_wrist\" in pose_dict and \\\n",
        "        \"left_shoulder\" in pose_dict and \"right_shoulder\" in pose_dict:\n",
        "          lw = pose_dict[\"left_wrist\"]\n",
        "          rw = pose_dict[\"right_wrist\"]\n",
        "          ls = pose_dict[\"left_shoulder\"]\n",
        "          rs = pose_dict[\"right_shoulder\"]\n",
        "          if lw['y'] < ls['y'] and rw['y'] < rs['y']:\n",
        "              return \"hands_up_pose\"\n",
        "\n",
        "      # Gun pointing - right arm extended forward (heuristic)\n",
        "      if \"right_wrist\" in pose_dict and \"right_elbow\" in pose_dict and \"right_shoulder\" in pose_dict:\n",
        "          rw = pose_dict[\"right_wrist\"]\n",
        "          re = pose_dict[\"right_elbow\"]\n",
        "          rs = pose_dict[\"right_shoulder\"]\n",
        "          if rw['x'] > re['x'] + 40 and abs(rw['y'] - re['y']) < 30 and abs(re['y'] - rs['y']) < 30:\n",
        "              return \"gun_pointing_pose\"\n",
        "\n",
        "      # Pointing pose - check both hands for consistent direction\n",
        "\n",
        "      import math\n",
        "\n",
        "      def distance(p1, p2):\n",
        "          return math.sqrt((p1[\"x\"] - p2[\"x\"])**2 + (p1[\"y\"] - p2[\"y\"])**2)\n",
        "\n",
        "      def angle_between(p1, p2, p3):\n",
        "          a = distance(p2, p3)\n",
        "          b = distance(p1, p3)\n",
        "          c = distance(p1, p2)\n",
        "          if a * c == 0:\n",
        "              return 180\n",
        "          cos_angle = (a**2 + c**2 - b**2) / (2 * a * c)\n",
        "          cos_angle = max(-1, min(1, cos_angle))\n",
        "          return math.degrees(math.acos(cos_angle))\n",
        "\n",
        "      def vector(p1, p2):\n",
        "          return (p2[\"x\"] - p1[\"x\"], p2[\"y\"] - p1[\"y\"])\n",
        "\n",
        "      def unit_vector(v):\n",
        "          mag = math.sqrt(v[0]*2 + v[1]*2)\n",
        "          if mag == 0:\n",
        "              return (0, 0)\n",
        "          return (v[0]/mag, v[1]/mag)\n",
        "\n",
        "      def dot(v1, v2):\n",
        "          return v1[0]*v2[0] + v1[1]*v2[1]\n",
        "      import math\n",
        "\n",
        "      def is_pointing_arm(shoulder, elbow, wrist, min_arm_length=80, min_angle=160, min_forward_x=30):\n",
        "          # Calculate arm lengths\n",
        "          upper_arm = distance(shoulder, elbow)\n",
        "          lower_arm = distance(elbow, wrist)\n",
        "          full_arm = distance(shoulder, wrist)\n",
        "\n",
        "          # Calculate angle between shoulder, elbow, and wrist\n",
        "          angle = angle_between(shoulder, elbow, wrist)\n",
        "\n",
        "          # Must be mostly straight\n",
        "          if angle < min_angle:\n",
        "              return False\n",
        "\n",
        "          # Must be long enough\n",
        "          if full_arm < min_arm_length:\n",
        "              return False\n",
        "\n",
        "          # Wrist should be in front of shoulder\n",
        "          if wrist[\"x\"] - shoulder[\"x\"] < min_forward_x:\n",
        "              return False\n",
        "\n",
        "          # NEW: Arm vector should not be pointing downward\n",
        "          dx = wrist[\"x\"] - shoulder[\"x\"]\n",
        "          dy = wrist[\"y\"] - shoulder[\"y\"]\n",
        "          arm_angle = math.degrees(math.atan2(dy, dx))  # angle in degrees\n",
        "\n",
        "          # Accept only horizontal to slightly upward arm angles (e.g., -60 to +60 degrees)\n",
        "          if arm_angle > 60 or arm_angle < -60:\n",
        "              return False\n",
        "\n",
        "          return True\n",
        "\n",
        "      # --- POINTING LOGIC ---\n",
        "      left_pointing = False\n",
        "      if all(k in pose_dict for k in [\"left_shoulder\", \"left_elbow\", \"left_wrist\"]):\n",
        "          # For left arm, wrist should be to the *left* of shoulder (i.e., smaller x value)\n",
        "          if is_pointing_arm(pose_dict[\"left_shoulder\"], pose_dict[\"left_elbow\"], pose_dict[\"left_wrist\"], min_forward_x=-30):\n",
        "              left_pointing = True\n",
        "\n",
        "      right_pointing = False\n",
        "      if all(k in pose_dict for k in [\"right_shoulder\", \"right_elbow\", \"right_wrist\"]):\n",
        "          # For right arm, wrist should be to the *right* of shoulder (i.e., larger x value)\n",
        "          if is_pointing_arm(pose_dict[\"right_shoulder\"], pose_dict[\"right_elbow\"], pose_dict[\"right_wrist\"], min_forward_x=30):\n",
        "              right_pointing = True\n",
        "\n",
        "      if left_pointing or right_pointing:\n",
        "          return \"pointing\"\n",
        "\n",
        "      # Loitering\n",
        "      x1, y1, x2, y2 = bbox\n",
        "      cx = int((x1 + x2) / 2)\n",
        "      cy = int((y1 + y2) / 2)\n",
        "      if is_loitering(track_id, (cx, cy)):\n",
        "          return \"loitering\"\n",
        "\n",
        "      return \"normal\"\n",
        "\n",
        "  # Video input/output setup\n",
        "  cap = cv2.VideoCapture(video_path_final)\n",
        "  fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "  print(f\"Original video FPS: {fps}\")\n",
        "\n",
        "  target_fps = 2\n",
        "  frame_skip = int(round(fps / target_fps))\n",
        "  print(f\"Processing every {frame_skip} frame(s) to get ~{target_fps} FPS\")\n",
        "\n",
        "  fourcc = cv2.VideoWriter_fourcc(*'mp4v')\n",
        "  width = int(cap.get(cv2.CAP_PROP_FRAME_WIDTH))\n",
        "  height = int(cap.get(cv2.CAP_PROP_FRAME_HEIGHT))\n",
        "  out = cv2.VideoWriter('output_annotated.mp4', fourcc, target_fps, (width, height))\n",
        "\n",
        "  log_file = open(\"behavior_log.txt\", \"w\")\n",
        "  text+=\"Pose+person tracking model's output\"\n",
        "  frame_count = 0\n",
        "\n",
        "  while True:\n",
        "      ret, frame = cap.read()\n",
        "      if not ret:\n",
        "          break\n",
        "\n",
        "      if frame_count % frame_skip == 0:\n",
        "          results = yolo(frame)[0]\n",
        "          detections = []\n",
        "\n",
        "          # Count detected classes for summary\n",
        "          class_counts = {}\n",
        "\n",
        "          for det in results.boxes.data.cpu().numpy():\n",
        "              x1, y1, x2, y2, conf, cls = det\n",
        "              cls = int(cls)\n",
        "              class_name = yolo.model.names[cls]\n",
        "\n",
        "              # Update count\n",
        "              class_counts[class_name] = class_counts.get(class_name, 0) + 1\n",
        "\n",
        "              # Your existing detection filtering for class 0 (person)\n",
        "              if cls == 0 and conf > 0.5:\n",
        "                  detections.append([x1, y1, x2, y2, conf])\n",
        "\n",
        "          # Build detection summary text\n",
        "          detection_summary = ', '.join(\n",
        "              f\"{count} {name}{'s' if count > 1 else ''}\"\n",
        "              for name, count in class_counts.items()\n",
        "          )\n",
        "\n",
        "          tracks = tracker.update(detections)\n",
        "          output_dict = {}\n",
        "          # text = \"\"  # reset per frame text info\n",
        "\n",
        "          for track in tracks:\n",
        "              x1, y1, x2, y2, track_id = map(int, track)\n",
        "              person_crop = frame[y1:y2, x1:x2]\n",
        "              h, w = person_crop.shape[:2]\n",
        "\n",
        "              if h == 0 or w == 0:\n",
        "                  continue\n",
        "\n",
        "              pose_result = mp_pose.process(cv2.cvtColor(person_crop, cv2.COLOR_BGR2RGB))\n",
        "              joint_data = {}\n",
        "\n",
        "              if pose_result.pose_landmarks:\n",
        "                  for idx, lm in enumerate(pose_result.pose_landmarks.landmark):\n",
        "                      joint_name = mp.solutions.pose.PoseLandmark(idx).name.lower()\n",
        "                      joint_data[joint_name] = {\n",
        "                          \"x\": int(x1 + lm.x * w),\n",
        "                          \"y\": int(y1 + lm.y * h),\n",
        "                          \"confidence\": round(lm.visibility, 2)\n",
        "                      }\n",
        "\n",
        "              behavior = classify_behavior(joint_data, (x1, y1, x2, y2), track_id)\n",
        "              output_dict[f\"id:{track_id}\"] = {\n",
        "                  # \"joints\": joint_data,\n",
        "                  \"behavior\": behavior\n",
        "              }\n",
        "              text += f\"{track_id}:{behavior}\\n\"\n",
        "\n",
        "              # Draw bounding box + label\n",
        "              color = (0, 255, 0)  # green default\n",
        "              if behavior in [\"collapsed\", \"pointing\", \"hands_up_pose\", \"gun_pointing_pose\", \"aggressive\", \"loitering\"]:\n",
        "                  color = (0, 0, 255)  # red alert\n",
        "\n",
        "              cv2.rectangle(frame, (x1, y1), (x2, y2), color, 2)\n",
        "              label = f\"ID:{track_id} {behavior}\"\n",
        "              cv2.putText(frame, label, (x1, y1 - 10),\n",
        "                          cv2.FONT_HERSHEY_SIMPLEX, 0.6, color, 2)\n",
        "\n",
        "          # Overlay detection summary on the frame (top-left corner)\n",
        "          if detection_summary:\n",
        "              cv2.putText(\n",
        "                  frame,\n",
        "                  detection_summary,\n",
        "                  (10, 30),\n",
        "                  cv2.FONT_HERSHEY_SIMPLEX,\n",
        "                  0.8,\n",
        "                  (255, 255, 255),\n",
        "                  2,\n",
        "                  cv2.LINE_AA\n",
        "              )\n",
        "\n",
        "          log_file.write(json.dumps(output_dict) + \"\\n\")\n",
        "          out.write(frame)\n",
        "          # cv2_imshow(frame)\n",
        "          # if cv2.waitKey(1) & 0xFF == ord('q'):\n",
        "          #     break\n",
        "      frame_count += 1\n",
        "\n",
        "  cap.release()\n",
        "  out.release()\n",
        "  log_file.close()\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "metadata": {
        "id": "sb6gjAUfJ65B"
      },
      "outputs": [],
      "source": [
        "import cv2\n",
        "from ultralytics import YOLO\n",
        "from collections import Counter\n",
        "def yolo_model(text,video_path_final):\n",
        "  # Load pretrained YOLOv8 model (you can use yolov8n, yolov8s, yolov8m, yolov8l, yolov8x)\n",
        "  model = YOLO(\"yolov8n.pt\")  # Lightweight model for speed\n",
        "  text+=\"yolo model outputs per frame:\\n\"\n",
        "  def analyze_video_yolo(video_path,text, target_fps=2):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "      frame_interval = int(original_fps // target_fps)\n",
        "      frame_index = 0\n",
        "      processed_frame = 0\n",
        "\n",
        "      while cap.isOpened():\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "\n",
        "          if frame_index % frame_interval == 0:\n",
        "              # Run YOLO inference on the frame\n",
        "              results = model.predict(source=frame, imgsz=640, conf=0.25, verbose=False)\n",
        "              names = model.model.names\n",
        "\n",
        "              object_counts = Counter()\n",
        "              for r in results:\n",
        "                  for cls in r.boxes.cls:\n",
        "                      label = names[int(cls)]\n",
        "                      object_counts[label] += 1\n",
        "\n",
        "              print(f\"Frame {processed_frame}: \", dict(object_counts))\n",
        "              text += f\"Frame {processed_frame}: \" + str(dict(object_counts)) + \"\\n\"\n",
        "              processed_frame += 1\n",
        "\n",
        "          frame_index += 1\n",
        "\n",
        "      cap.release()\n",
        "      return text\n",
        "\n",
        "  # ✅ Replace with your video path\n",
        "  text+=analyze_video_yolo(video_path_final,text)\n",
        "  return text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FgC9byz0dkaZ",
        "outputId": "0a69108a-0899-4ed7-f4c0-11e7a1b6c970"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:282: ModelDependencyMissing: Your `inference` configuration does not support PaliGemma model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:341: ModelDependencyMissing: Your `inference` configuration does not support Qwen2.5-VL model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:353: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:363: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[sam]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:374: ModelDependencyMissing: Your `inference` configuration does not support SAM model. Use pip install 'inference[clip]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:386: ModelDependencyMissing: Your `inference` configuration does not support OWLv2 model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:412: ModelDependencyMissing: Your `inference` configuration does not support SmolVLM2.Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:425: ModelDependencyMissing: Your `inference` configuration does not support Depth Estimation.Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:456: ModelDependencyMissing: Your `inference` configuration does not support TrOCR model. Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:467: ModelDependencyMissing: Your `inference` configuration does not support GroundingDINO model. Use pip install 'inference[grounding-dino]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:478: ModelDependencyMissing: Your `inference` configuration does not support YoloWorld model. Use pip install 'inference[yolo-world]' to install missing requirements.\n",
            "  warnings.warn(\n",
            "/usr/local/lib/python3.11/dist-packages/inference/models/utils.py:491: ModelDependencyMissing: Your `inference` configuration does not support Perception Encoder.Use pip install 'inference[transformers]' to install missing requirements.\n",
            "  warnings.warn(\n"
          ]
        }
      ],
      "source": [
        "from inference import get_model\n",
        "import cv2\n",
        "\n",
        "def weapon_detection_model(text, video_path_final):\n",
        "    # Load models\n",
        "    model1 = get_model(model_id=\"weapon-detection-aoxpz/5\", api_key=\"YOUR ROBOFLOW API KEY\")\n",
        "    model2 = get_model(model_id=\"knife_labels/2\", api_key=\"YOUR ROBOFLOW API KEY\")\n",
        "    model3 = get_model(model_id=\"blood-detection-2/1\", api_key=\"YOUR ROBOFLOW API KEY\")\n",
        "\n",
        "    # Load video\n",
        "    cap = cv2.VideoCapture(video_path_final)\n",
        "    fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "    frame_interval = int(fps / 2)\n",
        "\n",
        "    frame_count = 0\n",
        "    processed_frame_idx = 1\n",
        "\n",
        "    while cap.isOpened():\n",
        "        ret, frame = cap.read()\n",
        "        if not ret:\n",
        "            break\n",
        "\n",
        "        if frame_count % frame_interval == 0:\n",
        "            print(f\"\\n📸 Frame {processed_frame_idx}:\")\n",
        "            text += f\"\\nFrame {processed_frame_idx}:\\n\"\n",
        "\n",
        "            # Inference\n",
        "            results1 = model1.infer(frame)[0]\n",
        "            results2 = model2.infer(frame)[0]\n",
        "            results3 = model3.infer(frame)[0]\n",
        "\n",
        "            # Extract class names\n",
        "            classes1 = [pred.class_name for pred in results1.predictions]\n",
        "            classes2 = [pred.class_name for pred in results2.predictions]\n",
        "            classes3 = [pred.class_name for pred in results3.predictions]\n",
        "\n",
        "            # Print and update text\n",
        "            print(\"  Model 1 classes:\", classes1)\n",
        "            print(\"  Model 2 classes:\", classes2)\n",
        "            print(\"  Model 3 classes:\", classes3)\n",
        "\n",
        "            text += \"  Model 1 classes: \" + \", \".join(classes1) + \"\\n\"\n",
        "            text += \"  Model 2 classes: \" + \", \".join(classes2) + \"\\n\"\n",
        "            text += \"  Model 3 classes: \" + \", \".join(classes3) + \"\\n\"\n",
        "\n",
        "            processed_frame_idx += 1\n",
        "\n",
        "        frame_count += 1\n",
        "\n",
        "    cap.release()\n",
        "    cv2.destroyAllWindows()\n",
        "    return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 8,
      "metadata": {
        "id": "AGw1ACGLBqpT"
      },
      "outputs": [],
      "source": [
        "from ultralytics import YOLO\n",
        "import cv2\n",
        "def fire_detection(text,video_path_final):\n",
        "  def process_video(text,video_path, model_path='/content/best.pt', fps=2, conf=0.4):\n",
        "      model = YOLO(model_path)\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "\n",
        "      original_fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "      frame_interval = int(original_fps / fps) if original_fps else 15\n",
        "\n",
        "      frame_idx = 0\n",
        "      output_idx = 0\n",
        "\n",
        "      while True:\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "\n",
        "          if frame_idx % frame_interval == 0:\n",
        "              results = model(frame, conf=conf)[0]\n",
        "\n",
        "              # Extract detected class names\n",
        "              if results.boxes:\n",
        "                  class_ids = results.boxes.cls.cpu().numpy()\n",
        "                  class_names = [results.names[int(i)].lower() for i in class_ids]\n",
        "                  fire_detected = any(name in ['fire', 'smoke'] for name in class_names)\n",
        "              else:\n",
        "                  fire_detected = False\n",
        "\n",
        "              print(f\"frame {output_idx}: {'fire detected' if fire_detected else 'no fire'}\")\n",
        "              text+=f\"frame {output_idx}: {'fire detected' if fire_detected else 'no fire'}\"\n",
        "              output_idx += 1\n",
        "\n",
        "          frame_idx += 1\n",
        "\n",
        "      cap.release()\n",
        "      return text\n",
        "  text=process_video(text,video_path_final, model_path='/content/best.pt', fps=2, conf=0.4)\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "WLE6DGFTuSQQ",
        "outputId": "9e79e5df-6789-4759-8a60-5edcc04e352f"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Using cache found in /root/.cache/torch/hub/ultralytics_yolov5_master\n",
            "YOLOv5 🚀 2025-6-23 Python-3.11.13 torch-2.2.2+cu121 CUDA:0 (Tesla T4, 15095MiB)\n",
            "\n",
            "Fusing layers... \n",
            "YOLOv5s summary: 213 layers, 7225885 parameters, 0 gradients, 16.4 GFLOPs\n",
            "Adding AutoShape... \n"
          ]
        }
      ],
      "source": [
        "import cv2\n",
        "import torch\n",
        "from IPython.display import display\n",
        "import PIL.Image\n",
        "import numpy as np\n",
        "\n",
        "# === Load YOLOv5 model (ultralytics version) ===\n",
        "model = torch.hub.load('ultralytics/yolov5', 'yolov5s', pretrained=True)\n",
        "model.conf = 0.3  # Confidence threshold\n",
        "\n",
        "def age_detection(text,video_path_final):\n",
        "# === Show frame in Colab ===\n",
        "  def show_frame(frame, title=\"Frame\"):\n",
        "      rgb = cv2.cvtColor(frame, cv2.COLOR_BGR2RGB)\n",
        "      img_pil = PIL.Image.fromarray(rgb)\n",
        "      display(img_pil)\n",
        "\n",
        "  # === Age Estimation Based on Bounding Box ===\n",
        "  def estimate_age_from_relative_height(box_height, box_width, image_height, cctv_mode=False):\n",
        "      ratio = box_height / image_height\n",
        "      aspect_ratio = box_height / box_width if box_width > 0 else 1e9\n",
        "\n",
        "      if cctv_mode:\n",
        "          if box_height < 200:\n",
        "              return 30, \"Adult (CCTV)\"\n",
        "          elif 200 <= box_height < 240:\n",
        "              return 18, \"Teen (CCTV)\"\n",
        "          elif 240 <= box_height < 280:\n",
        "              return 10, \"Child (CCTV)\"\n",
        "          else:\n",
        "              return 30, \"Adult (CCTV+Large)\"\n",
        "\n",
        "      if box_height > 320:\n",
        "          return 30, \"Adult\"\n",
        "      if box_height > 160 and box_height <= 320 and aspect_ratio < 1.6:\n",
        "          return 18, \"Teen (Half-Body)\"\n",
        "      if ratio > 0.52 and box_height > 200:\n",
        "          return 30, \"Adult\"\n",
        "      elif 0.38 < ratio <= 0.52 and box_height > 90:\n",
        "          return 18, \"Teen\"\n",
        "      elif 0.22 < ratio <= 0.35:\n",
        "          return 10, \"Child\"\n",
        "      elif 0.12 < ratio <= 0.22:\n",
        "          return 5, \"Toddler\"\n",
        "      else:\n",
        "          return 2, \"Infant\"\n",
        "\n",
        "  # === Main Processing Function ===\n",
        "  def process_video_with_display(text,video_path):\n",
        "      cap = cv2.VideoCapture(video_path)\n",
        "      fps = cap.get(cv2.CAP_PROP_FPS)\n",
        "      interval = int(fps // 2)  # 2 frames per second\n",
        "\n",
        "      frame_count = 0\n",
        "      displayed = 0\n",
        "\n",
        "      print(\"\\n🎞 Processing video and displaying results...\\n\")\n",
        "\n",
        "      while cap.isOpened():\n",
        "          ret, frame = cap.read()\n",
        "          if not ret:\n",
        "              break\n",
        "\n",
        "          if frame_count % interval == 0:\n",
        "              results = model(frame)\n",
        "              detections = results.pred[0]\n",
        "              people_detections = [d for d in detections if int(d[-1]) == 0]\n",
        "\n",
        "              small_people_count = 0\n",
        "              for person_det in people_detections:\n",
        "                  x1, y1, x2, y2 = map(int, person_det[:4])\n",
        "                  height = y2 - y1\n",
        "                  if height < 140:\n",
        "                      small_people_count += 1\n",
        "              cctv_mode = small_people_count >= 2\n",
        "              # print(f\"🟡 Detected {small_people_count} small people. CCTV mode: {cctv_mode}\")\n",
        "\n",
        "              # Initialize counters\n",
        "              adult_count = 0\n",
        "              teen_count = 0\n",
        "              child_count = 0\n",
        "\n",
        "              for person_det in people_detections:\n",
        "                  x1, y1, x2, y2 = map(int, person_det[:4])\n",
        "                  height = y2 - y1\n",
        "                  width = x2 - x1\n",
        "                  image_height = frame.shape[0]\n",
        "\n",
        "                  est_age, category = estimate_age_from_relative_height(height, width, image_height, cctv_mode)\n",
        "\n",
        "                  # Count based on category\n",
        "                  if \"Adult\" in category:\n",
        "                      adult_count += 1\n",
        "                  elif \"Teen\" in category:\n",
        "                      teen_count += 1\n",
        "                  elif \"Child\" in category:\n",
        "                      child_count += 1\n",
        "\n",
        "                  label = f\"{category}, Age ~ {est_age}\"\n",
        "                  cv2.rectangle(frame, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
        "                  cv2.putText(frame, label, (x1, y1 - 10),\n",
        "                              cv2.FONT_HERSHEY_SIMPLEX, 0.6, (0, 255, 0), 2)\n",
        "\n",
        "              # Summary Output\n",
        "              print(f\"frame: {displayed+1}  adults: {adult_count}  teen: {teen_count}  child: {child_count}\")\n",
        "              text+=f\"frame: {displayed+1}  adults: {adult_count}  teen: {teen_count}  child: {child_count}\"\n",
        "              # text+=f\"frame: {displayed+1}  adults: 0  teen: 0  child: 1\"\n",
        "              # show_frame(frame, title=f\"Frame {displayed+1}\")\n",
        "              displayed += 1\n",
        "\n",
        "          frame_count += 1\n",
        "\n",
        "      cap.release()\n",
        "      return text\n",
        "  text=process_video_with_display(text,video_path_final)\n",
        "  return text\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "Swr-Gzxnq3cI",
        "outputId": "d4431d3f-ea75-4e28-883d-472212153586"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Crime-related predictions (sorted):\n",
            "kicking soccer ball: 12.85%\n",
            "drop kicking: 12.48%\n",
            "sword fighting: 12.46%\n",
            "kicking field goal: 12.45%\n",
            "punching person (boxing): 12.45%\n",
            "arm wrestling: 12.44%\n",
            "wrestling: 12.44%\n",
            "punching bag: 12.44%\n",
            "[Audio Error] ffmpeg error (see stderr output for detail)\n",
            "Original video FPS: 25.0\n",
            "Processing every 12 frame(s) to get ~2 FPS\n",
            "\n",
            "0: 384x640 4 persons, 24.0ms\n",
            "Speed: 3.5ms preprocess, 24.0ms inference, 4.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 kite, 11.4ms\n",
            "Speed: 2.9ms preprocess, 11.4ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 12.6ms\n",
            "Speed: 2.9ms preprocess, 12.6ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 frisbee, 11.9ms\n",
            "Speed: 5.9ms preprocess, 11.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 3 sports balls, 14.1ms\n",
            "Speed: 3.1ms preprocess, 14.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 baseball glove, 13.8ms\n",
            "Speed: 3.5ms preprocess, 13.8ms inference, 2.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 3 sports balls, 17.8ms\n",
            "Speed: 3.3ms preprocess, 17.8ms inference, 4.2ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 19.0ms\n",
            "Speed: 3.1ms preprocess, 19.0ms inference, 2.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 20.3ms\n",
            "Speed: 3.2ms preprocess, 20.3ms inference, 2.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.8ms\n",
            "Speed: 3.9ms preprocess, 9.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 2.8ms preprocess, 7.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.3ms\n",
            "Speed: 4.0ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.5ms\n",
            "Speed: 3.3ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 1 baseball glove, 9.5ms\n",
            "Speed: 3.4ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.6ms\n",
            "Speed: 3.1ms preprocess, 8.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 10.8ms\n",
            "Speed: 4.3ms preprocess, 10.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 8.4ms\n",
            "Speed: 3.3ms preprocess, 8.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.5ms\n",
            "Speed: 2.4ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 frisbee, 1 sports ball, 10.1ms\n",
            "Speed: 3.6ms preprocess, 10.1ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.1ms\n",
            "Speed: 2.1ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 handbag, 1 kite, 1 baseball glove, 9.1ms\n",
            "Speed: 3.3ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 6 persons, 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 9.2ms\n",
            "Speed: 3.0ms preprocess, 9.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 kite, 7.5ms\n",
            "Speed: 2.3ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 baseball glove, 8.5ms\n",
            "Speed: 3.5ms preprocess, 8.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 2.8ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 8.5ms\n",
            "Speed: 3.4ms preprocess, 8.5ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 9.1ms\n",
            "Speed: 3.5ms preprocess, 9.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 2 baseball gloves, 8.0ms\n",
            "Speed: 2.9ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 7.9ms\n",
            "Speed: 2.9ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 baseball glove, 8.1ms\n",
            "Speed: 4.7ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 8.3ms\n",
            "Speed: 3.4ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.5ms\n",
            "Speed: 3.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 baseball glove, 7.2ms\n",
            "Speed: 2.8ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 dog, 10.5ms\n",
            "Speed: 5.4ms preprocess, 10.5ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.9ms\n",
            "Speed: 3.0ms preprocess, 7.9ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 14.9ms\n",
            "Speed: 3.1ms preprocess, 14.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 baseball glove, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.7ms\n",
            "Speed: 3.2ms preprocess, 8.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.8ms\n",
            "Speed: 3.6ms preprocess, 7.8ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.4ms\n",
            "Speed: 3.2ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9.5ms\n",
            "Speed: 3.6ms preprocess, 9.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.7ms\n",
            "Speed: 3.3ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 7.7ms\n",
            "Speed: 3.1ms preprocess, 7.7ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9.9ms\n",
            "Speed: 3.4ms preprocess, 9.9ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 8.0ms\n",
            "Speed: 4.7ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.0ms\n",
            "Speed: 4.1ms preprocess, 8.0ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 7.3ms\n",
            "Speed: 2.4ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 1 sports ball, 8.3ms\n",
            "Speed: 4.0ms preprocess, 8.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 7.4ms\n",
            "Speed: 2.1ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 sports ball, 8.9ms\n",
            "Speed: 3.6ms preprocess, 8.9ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 3 sports balls, 8.1ms\n",
            "Speed: 3.8ms preprocess, 8.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 7.3ms\n",
            "Speed: 3.2ms preprocess, 7.3ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 1 kite, 17.7ms\n",
            "Speed: 2.9ms preprocess, 17.7ms inference, 3.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 3 persons, 14.8ms\n",
            "Speed: 2.8ms preprocess, 14.8ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9.9ms\n",
            "Speed: 3.0ms preprocess, 9.9ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 frisbee, 2 sports balls, 13.7ms\n",
            "Speed: 3.0ms preprocess, 13.7ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 14.5ms\n",
            "Speed: 2.9ms preprocess, 14.5ms inference, 1.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 9.2ms\n",
            "Speed: 3.5ms preprocess, 9.2ms inference, 1.9ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 1 sports ball, 9.1ms\n",
            "Speed: 2.9ms preprocess, 9.1ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.1ms\n",
            "Speed: 6.0ms preprocess, 8.1ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 7.6ms\n",
            "Speed: 2.8ms preprocess, 7.6ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 3 sports balls, 7.4ms\n",
            "Speed: 2.4ms preprocess, 7.4ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 4 persons, 8.2ms\n",
            "Speed: 4.1ms preprocess, 8.2ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "\n",
            "0: 384x640 5 persons, 1 sports ball, 15.0ms\n",
            "Speed: 6.1ms preprocess, 15.0ms inference, 2.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "Frame 0:  {'person': 4}\n",
            "Frame 1:  {'person': 4, 'kite': 1}\n",
            "Frame 2:  {'person': 4, 'sports ball': 1}\n",
            "Frame 3:  {'person': 4, 'frisbee': 1}\n",
            "Frame 4:  {'person': 4, 'sports ball': 3}\n",
            "Frame 5:  {'person': 4, 'baseball glove': 1}\n",
            "Frame 6:  {'person': 4, 'sports ball': 3}\n",
            "Frame 7:  {'person': 4}\n",
            "Frame 8:  {'person': 3}\n",
            "Frame 9:  {'person': 3}\n",
            "Frame 10:  {'person': 3}\n",
            "Frame 11:  {'person': 3}\n",
            "Frame 12:  {'person': 4}\n",
            "Frame 13:  {'person': 4, 'sports ball': 1, 'baseball glove': 1}\n",
            "Frame 14:  {'person': 4}\n",
            "Frame 15:  {'person': 4, 'sports ball': 1}\n",
            "Frame 16:  {'person': 4, 'sports ball': 1}\n",
            "Frame 17:  {'person': 4}\n",
            "Frame 18:  {'person': 4}\n",
            "Frame 19:  {'person': 4, 'sports ball': 1, 'frisbee': 1}\n",
            "Frame 20:  {'person': 3}\n",
            "Frame 21:  {'person': 3}\n",
            "Frame 22:  {'person': 4, 'baseball glove': 1, 'kite': 1, 'handbag': 1}\n",
            "Frame 23:  {'person': 6}\n",
            "Frame 24:  {'person': 3}\n",
            "Frame 25:  {'person': 4, 'kite': 1}\n",
            "Frame 26:  {'person': 4}\n",
            "Frame 27:  {'person': 4, 'baseball glove': 1}\n",
            "Frame 28:  {'person': 4}\n",
            "Frame 29:  {'person': 4, 'sports ball': 1}\n",
            "Frame 30:  {'person': 4}\n",
            "Frame 31:  {'person': 4, 'sports ball': 1}\n",
            "Frame 32:  {'person': 5, 'baseball glove': 2}\n",
            "Frame 33:  {'person': 5}\n",
            "Frame 34:  {'person': 4, 'baseball glove': 1}\n",
            "Frame 35:  {'person': 3}\n",
            "Frame 36:  {'person': 3}\n",
            "Frame 37:  {'person': 3, 'baseball glove': 1}\n",
            "Frame 38:  {'person': 3, 'dog': 1}\n",
            "Frame 39:  {'person': 3}\n",
            "Frame 40:  {'person': 4}\n",
            "Frame 41:  {'person': 4, 'baseball glove': 1}\n",
            "Frame 42:  {'person': 4}\n",
            "Frame 43:  {'person': 3}\n",
            "Frame 44:  {'person': 4}\n",
            "Frame 45:  {'person': 4}\n",
            "Frame 46:  {'person': 4}\n",
            "Frame 47:  {'person': 4, 'sports ball': 1}\n",
            "Frame 48:  {'person': 4}\n",
            "Frame 49:  {'person': 4, 'sports ball': 1}\n",
            "Frame 50:  {'person': 4}\n",
            "Frame 51:  {'person': 4, 'sports ball': 1}\n",
            "Frame 52:  {'person': 5, 'sports ball': 1}\n",
            "Frame 53:  {'sports ball': 1, 'person': 4}\n",
            "Frame 54:  {'sports ball': 1, 'person': 3}\n",
            "Frame 55:  {'sports ball': 3, 'person': 4}\n",
            "Frame 56:  {'person': 3}\n",
            "Frame 57:  {'person': 3, 'kite': 1}\n",
            "Frame 58:  {'person': 3}\n",
            "Frame 59:  {'person': 4}\n",
            "Frame 60:  {'person': 4, 'sports ball': 2, 'frisbee': 1}\n",
            "Frame 61:  {'person': 4}\n",
            "Frame 62:  {'person': 4}\n",
            "Frame 63:  {'person': 4, 'sports ball': 1}\n",
            "Frame 64:  {'person': 4}\n",
            "Frame 65:  {'person': 4}\n",
            "Frame 66:  {'person': 4, 'sports ball': 3}\n",
            "Frame 67:  {'person': 4}\n",
            "Frame 68:  {'person': 5, 'sports ball': 1}\n",
            "\n",
            "0: 384x640 (no detections), 7.1ms\n",
            "Speed: 1.9ms preprocess, 7.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 0: no fire\n",
            "\n",
            "0: 384x640 1 fire, 6.8ms\n",
            "Speed: 3.1ms preprocess, 6.8ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 1: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 6.7ms\n",
            "Speed: 2.0ms preprocess, 6.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 2: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 3: no fire\n",
            "\n",
            "0: 384x640 1 fire, 8.5ms\n",
            "Speed: 2.6ms preprocess, 8.5ms inference, 1.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 4: fire detected\n",
            "\n",
            "0: 384x640 1 fire, 7.8ms\n",
            "Speed: 3.8ms preprocess, 7.8ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 5: fire detected\n",
            "\n",
            "0: 384x640 1 fire, 7.5ms\n",
            "Speed: 2.2ms preprocess, 7.5ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 6: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 7.7ms\n",
            "Speed: 3.4ms preprocess, 7.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 7: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.7ms\n",
            "Speed: 3.9ms preprocess, 8.7ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 8: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.6ms\n",
            "Speed: 4.2ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 9: no fire\n",
            "\n",
            "0: 384x640 (no detections), 10.6ms\n",
            "Speed: 3.3ms preprocess, 10.6ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 10: no fire\n",
            "\n",
            "0: 384x640 (no detections), 11.3ms\n",
            "Speed: 5.4ms preprocess, 11.3ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 11: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 12: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 4.1ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 13: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.6ms\n",
            "Speed: 2.9ms preprocess, 8.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 14: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.1ms\n",
            "Speed: 3.6ms preprocess, 8.1ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 15: no fire\n",
            "\n",
            "0: 384x640 (no detections), 9.4ms\n",
            "Speed: 2.9ms preprocess, 9.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 16: no fire\n",
            "\n",
            "0: 384x640 1 fire, 8.6ms\n",
            "Speed: 4.7ms preprocess, 8.6ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 17: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 18: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 19: no fire\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.9ms preprocess, 6.8ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 20: no fire\n",
            "\n",
            "0: 384x640 (no detections), 15.1ms\n",
            "Speed: 2.9ms preprocess, 15.1ms inference, 1.1ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 21: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 3.3ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 22: no fire\n",
            "\n",
            "0: 384x640 1 fire, 7.3ms\n",
            "Speed: 3.3ms preprocess, 7.3ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 23: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 24: no fire\n",
            "\n",
            "0: 384x640 (no detections), 9.8ms\n",
            "Speed: 3.8ms preprocess, 9.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 25: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.9ms\n",
            "Speed: 2.2ms preprocess, 7.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 26: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 3.1ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 27: no fire\n",
            "\n",
            "0: 384x640 1 fire, 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 28: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 3.3ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 29: no fire\n",
            "\n",
            "0: 384x640 1 fire, 7.4ms\n",
            "Speed: 3.4ms preprocess, 7.4ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 30: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 3.1ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 31: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 32: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 3.7ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 33: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 3.2ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 34: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 35: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.3ms\n",
            "Speed: 2.8ms preprocess, 8.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 36: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 3.9ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 37: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 2.9ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 38: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 3.0ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 39: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 3.4ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 40: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 6.0ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 41: no fire\n",
            "\n",
            "0: 384x640 1 fire, 8.0ms\n",
            "Speed: 3.0ms preprocess, 8.0ms inference, 1.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 42: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 3.1ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 43: no fire\n",
            "\n",
            "0: 384x640 1 fire, 7.0ms\n",
            "Speed: 3.6ms preprocess, 7.0ms inference, 1.4ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 44: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 7.4ms\n",
            "Speed: 3.0ms preprocess, 7.4ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 45: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 3.2ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 46: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 3.0ms preprocess, 7.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 47: no fire\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 3.2ms preprocess, 6.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 48: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 4.7ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 49: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 2.9ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 50: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.0ms\n",
            "Speed: 2.8ms preprocess, 7.0ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 51: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.2ms\n",
            "Speed: 3.2ms preprocess, 8.2ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 52: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 4.0ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 53: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 3.5ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 54: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 3.2ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 55: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.5ms\n",
            "Speed: 3.8ms preprocess, 7.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 56: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 3.3ms preprocess, 8.0ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 57: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.1ms\n",
            "Speed: 3.7ms preprocess, 8.1ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 58: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.3ms\n",
            "Speed: 2.7ms preprocess, 7.3ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 59: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.6ms\n",
            "Speed: 3.5ms preprocess, 7.6ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 60: no fire\n",
            "\n",
            "0: 384x640 1 fire, 7.2ms\n",
            "Speed: 2.2ms preprocess, 7.2ms inference, 1.5ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 61: fire detected\n",
            "\n",
            "0: 384x640 (no detections), 7.2ms\n",
            "Speed: 3.3ms preprocess, 7.2ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 62: no fire\n",
            "\n",
            "0: 384x640 (no detections), 10.5ms\n",
            "Speed: 2.9ms preprocess, 10.5ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 63: no fire\n",
            "\n",
            "0: 384x640 (no detections), 8.0ms\n",
            "Speed: 3.1ms preprocess, 8.0ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 64: no fire\n",
            "\n",
            "0: 384x640 (no detections), 6.8ms\n",
            "Speed: 2.9ms preprocess, 6.8ms inference, 0.6ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 65: no fire\n",
            "\n",
            "0: 384x640 (no detections), 6.9ms\n",
            "Speed: 3.2ms preprocess, 6.9ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 66: no fire\n",
            "\n",
            "0: 384x640 (no detections), 14.3ms\n",
            "Speed: 4.2ms preprocess, 14.3ms inference, 1.0ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 67: no fire\n",
            "\n",
            "0: 384x640 (no detections), 7.8ms\n",
            "Speed: 2.9ms preprocess, 7.8ms inference, 0.7ms postprocess per image at shape (1, 3, 384, 640)\n",
            "frame 68: no fire\n",
            "```json\n",
            "{\n",
            "  \"raise_alarm\": true,\n",
            "  \"reason\": \"The action recognition model's top two predictions were 'kicking soccer ball' and 'drop kicking', which are not inherently violent. However, the pose detection model frequently identified 'hands_up_pose', which could indicate surrender or distress in the context of other potential threats.  Combined with the presence of fire (detected in multiple frames) and the YOLO model detecting a person consistently, this creates a situation requiring further investigation. The weapon detection model's results are inconclusive as less than 30% of the frames showed weapons which is below the threshold.  Therefore, an alarm should be raised due to the potential for danger based on the combination of factors.\"\n",
            "}\n",
            "```\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import google.generativeai as genai\n",
        "\n",
        "# Configure Gemini API\n",
        "genai.configure(api_key=\"YOUR GOOGLE GEMINI API KEY\")\n",
        "text=\"\"\n",
        "video_path_final=\"YOUR VIDEO PATH HERE\"\n",
        "def final_result(text):\n",
        "    if not text.strip():\n",
        "        return \"Neutral\"\n",
        "    prompt = f\"\"\"\n",
        "    Analyze the following the outputs from action recognition model, audio classification model, pose detection+person tracking model, yolo object detection model, weapon detection model,fire detection model.\n",
        "    action recognition model have around 400 labels out of which best labels are being extracted. *dont see the confidence score only consider the order and then give the result*.\n",
        "\n",
        "    give the output as raise alarm or don't raise alarm based on the above outputs.\n",
        "    take all the things into consideration and give me the final result.\n",
        "\n",
        "    if pose detection model is heavily recognizing anything then take that thing into consideration.\n",
        "    NOTE::the pose detection cannot be truly trusted just take it as an assistant for action recognition.\n",
        "    in the action recognition take the first 2 things into consideration and **strictly** not others and give the result accordingly.\n",
        "\n",
        "    take into account the yolo model's object detection output.\n",
        "\n",
        "    weapon detection models output would be 1st model would give guns or knifes,2nd model would give knife,3rd model would give blood stains 1 means blood stains 0 means no blood stain.\n",
        "    NOTE::if the weapon detection model's output is present strictly below 30 percent of the total number of frames which is 40 then dont take that particular thing into consideration as it can be a wrong detection.\n",
        "    the output of fire detection model should be taken into consideration.\n",
        "    give the result in an object form first should be raise an alarm or dont raise an alarm and second must be the reason\n",
        "\n",
        "    outputs: {text}\n",
        "    \"\"\"\n",
        "    model = genai.GenerativeModel(\"models/gemini-1.5-flash-latest\")\n",
        "    response = model.generate_content(prompt)\n",
        "    return response\n",
        "# take into account the age classification model's output.\n",
        "# give me the final result by taking the age of the persons.\n",
        "# take into account certain situations like if a gun or rifle and a child is present it may be saying that the child is using a rifle so it maybe a toy rifle.\n",
        "text=action_recognition_model(text,video_path_final)\n",
        "text=audio_classification_model(text,video_path_final)\n",
        "text=pose_person_model(text,video_path_final)\n",
        "text=yolo_model(text,video_path_final)\n",
        "text=weapon_detection_model(text,video_path_final)\n",
        "text=age_detection(text, video_path_final)\n",
        "text=fire_detection(text,video_path_final)\n",
        "res=final_result(text)\n",
        "print(res.text)"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
